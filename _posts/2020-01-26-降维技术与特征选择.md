---
date: 2020-01-26 19:24:49
layout: post
title: 降维技术与特征选择
description: 降维技术与特征选择
image: /post_images/ml/降维技术与特征选择封面.png
optimized_image: /post_images/ml/降维技术与特征选择封面.png
category: 机器学习
tags:
  - 机器学习
  - machine learning
  - 降维
  - 特征选择
  - 皮尔逊相关系数
  - PCA算法
  - 卡方检验
  - 余弦相似度
author: 沙中世界
---

### 特征相关性分析
可以理解为列向量的相似性分析，常见方法如下：<br>
**欧式距离：**

**余弦相似度：**计算向量夹角的余弦值。余弦相似度更多的是在衡量泛型的相似性，而对绝对值不敏感，比如：用户A对两部电影评分为1，2；用户B对两个电影评分为6和10；两个向量[1,2] 和 [6,10]的余弦相似度比较高，但明显A不喜欢这两部电影，B相对更喜欢一些；解决方法是：首先计算向量各个维度的均值（从列得维度进行中性化），然后减去均值；再计算相似度。

**皮尔逊相关系数：**实际上是向量半标准化（只减去自己的均值，从行得维度进行中性化）之后的余弦相似度，一般只可以衡量线性相关性；

**杰卡德相似系数：**用于向量元素是bool值得情况

相似度的计算是可以多种方法混合使用，举例：<br>
如何做社交网络好友相似的度量，粗略来说这几个特征：帖子发布数量，月均发帖数量，平均帖子字数，头像，一些标签数据，例如是否大V，是否营销号，是否网红，职业等标签数据。另外还可以统计发文Top关键词向量及词频。<br>
标签数据可计算杰卡的相似度，Top关键词可计算余弦相似度，发布量，字数等可计算欧氏距离，然后再融合这几种相似度得到总和相似度。


### PCA降维
主成成分分析，是一种线性降维技术;

PCA的两种通俗理解：1)是最大化投影后数据的方差(让数据更分散)；2)是最小化投影造成的损失。<br>
PCA降维可以一定程度上减少一些噪点特征，提高信噪比；

两种实现原理：基于特征协方差矩阵的特征值和特征向量进行降维 & 基于SVD(奇异值分解)进行降维

特征值与特征向量的数学计算：

奇异值分解的数学计算：

sklearn源码：
```python
    def fit_transform(self, X, y=None):
        U, S, V = self._fit(X)
        U = U[:, :self.n_components_]

        if self.whiten:
            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)
            U *= sqrt(X.shape[0] - 1)
        else:
            # X_new = X * V = U * S * V^T * V = U * S
            U *= S[:self.n_components_]

        return U
```

### LDA降维
有监督的线性降维算法

### MDS降维

### TSNE

### 卡方检验

### 信息增益
