---
date: 2024-09-08 16:50:48
layout: post
title: Hello,强化学习
description: Hello,强化学习
image: /post_images/rl/CartPole-v1.png
optimized_image: /post_images/rl/CartPole-v1.png
category: 机器学习
tags:
  - 机器学习
  - 强化学习
  - Q-learning
  - CartPole-v1.png
author: 沙中世界
---

python gym的基础知识以及CartPole-v1的环境、状态等信息说明，查阅官网介绍即可；

base规则策略，通常只能坚持30步左右，杆就会失去平衡，达到fail状态；

基于q-learning的参数更新，训练100步以下 基本没什么效果，但是训练到150 ~ 200步左右的时候，大多数时候可以稳定在100步以上；

```
...
epoch =  97 g_reward =  50.0
epoch =  98 g_reward =  59.0
epoch =  99 g_reward =  100.0
epoch =  100 g_reward =  28.0
epoch =  101 g_reward =  19.0
epoch =  102 g_reward =  57.0
epoch =  103 g_reward =  23.0
epoch =  104 g_reward =  17.0
epoch =  105 g_reward =  62.0
...
...
epoch =  182 g_reward =  100.0
epoch =  183 g_reward =  100.0
epoch =  184 g_reward =  100.0
epoch =  185 g_reward =  100.0
epoch =  186 g_reward =  100.0
epoch =  187 g_reward =  100.0
epoch =  188 g_reward =  30.0
epoch =  189 g_reward =  100.0
epoch =  190 g_reward =  100.0
epoch =  191 g_reward =  100.0
epoch =  192 g_reward =  100.0
epoch =  193 g_reward =  100.0
```
原理介绍：
Q-Learning的核心Q-table参数值以及如何计算得到这些参数值；<br>
Q-table是一个二维数组，一个维度表示状态，另一个维度表示动作，值表示在某种状态下，采取某种动作，所得到的回报；<br>
参数计算有多种方案，下面展示了其中一种，有两个核心点：<br>
① 通过随机选择action来探索一定状态下最合适的action；<br>
② 从回报函数的设计，后向的state-action会间接影响前向状态及action，也就是假定后向选择逻辑一定的情况下，当前action的选择如果正确，得到的回报会比较大，如果当前action选择错误，得到的回报会比较小；<br>
  如何实现这种效果呢？是因为当前action执行之后，会返回一个state，这个state对应一个Q-value，即：回报值；实际上是取的这个state对应的所有action中Q value的最大值；<br>
  这个回报值越大，则当前的state-action对应的值增加的越大，


完整可执行代码如下：
```python
import gym
import time
import numpy as np
from sklearn.preprocessing import KBinsDiscretizer
import time, math, random
from typing import Tuple

# 参考：https://github.com/RJBrooker/Q-learning-demo-Cartpole-V1/blob/master/cartpole.ipynb
env = gym.make('CartPole-v1', render_mode='human')
print(env.observation_space)
print(env.action_space)


# base 策略，只能支撑三四十步左右
def base_policy(obs):
    pos, velocity, angle, angle_velocity = obs
    return 1 if angle > 0 else 0


n_bins = (6, 12)
lower_bounds = [env.observation_space.low[2], -math.radians(50)]
upper_bounds = [env.observation_space.high[2], math.radians(50)]


def discretizer(_, __, angle, pole_velocity) -> Tuple[int, ...]:
    """Convert continues state intro a discrete state"""
    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')
    est.fit([lower_bounds, upper_bounds])
    return tuple(map(int, est.transform([[angle, pole_velocity]])[0]))


Q_table = np.zeros(n_bins + (env.action_space.n,))

# 训练100轮以下，看运气，运气差的时候 基本没什么效果
# 训练200轮以上，基本可以稳定支撑100步以上
def q_policy(state: tuple):
    """Choosing action based on epsilon-greedy policy"""
    return np.argmax(Q_table[state])


def new_Q_value(reward: float, new_state: tuple, discount_factor=1) -> float:
    """Temperal diffrence for updating Q-value of state-action pair"""
    future_optimal_value = np.max(Q_table[new_state])
    learned_value = reward + discount_factor * future_optimal_value
    return learned_value


def learning_rate(n: int, min_rate=0.01) -> float:
    """Decaying learning rate"""
    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))


def exploration_rate(n: int, min_rate=0.1) -> float:
    """Decaying exploration rate"""
    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))


train_count = 1000
e = 1
for epoch in range(train_count):
    g_reward = 0
    obs, _ = env.reset()
    current_state = discretizer(*obs)
    env.render()
    for _ in range(100):
        action = q_policy(current_state)
        if np.random.random() < exploration_rate(epoch):
            action = env.action_space.sample()  # explore
        obs, reward, terminated, truncated, info = env.step(action)
        new_state = discretizer(*obs)
        # Update Q-Table
        lr = learning_rate(epoch)
        learnt_value = new_Q_value(reward, new_state)
        old_value = Q_table[current_state][action]
        Q_table[current_state][action] = (1 - lr) * old_value + lr * learnt_value
        current_state = new_state

        g_reward += reward
        env.render()
        if terminated or truncated:
            print('epoch = ', epoch, 'g_reward = ', g_reward)
            break
        if g_reward == 100:
            print('epoch = ', epoch, 'g_reward = ', g_reward)
        # time.sleep(0.01)

env.close()

```