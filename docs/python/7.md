pytorch笔记
====
1. torch.device('cuda' if torch.cuda.is_available() else 'cpu') <br>
表示一个计算设备，后面可用于数据和模型计算，代码如下：
``` Python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data = data.to(device)
model = Model(...).to(device)
# 单机多卡的情况
if torch.cuda.device_count() > 1:
  model = nn.DataParallel(model，device_ids=[0,1,2])
model.to(device)
```
2. model.eval() 不启用batchNormalization 和 Dropout；相对应的model.train()启用归一化和Dropout；
3. with torch.no_grad 或者 @torch.no_grad()的作用，不计算梯度，反向传播不求导，通常用于模型test阶段，这样可以省去一些存储（因为如果要计算梯度，在前向传播时需要存储一些数据）；
4. torch torch.utils.data.Dataset（InMemoryDataset）函数，往往和自定义的dataSet一起使用；具体用法？？
5. torch.nn.Identity()，恒等变换，一般作为占位符使用；
6. torch_geometric的使用，一般用于图神经网络；
7. torch.manual_seed，设定随机数种子，多次运行程序时，可固定网络初始化；
