pytorch笔记
====
1. torch.device('cuda' if torch.cuda.is_available() else 'cpu') <br>
表示一个计算设备，后面可用于数据和模型计算，代码如下：
``` Python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data = data.to(device)
model = Model(...).to(device)
# 单机多卡的情况
if torch.cuda.device_count() > 1:
  model = nn.DataParallel(model，device_ids=[0,1,2])
model.to(device)
```
2. model.eval() 不启用batchNormalization 和 Dropout；相对应的model.train()启用归一化和Dropout；
3. with torch.no_grad 或者 @torch.no_grad()的作用，不计算梯度，反向传播不求导，通常用于模型test阶段，这样可以省去一些存储（因为如果要计算梯度，在前向传播时需要存储一些数据）；
4. torch torch.utils.data.Dataset（InMemoryDataset）函数，往往和自定义的dataSet一起使用；具体用法？？
5. torch.nn.Identity()，恒等变换，一般作为占位符使用；
6. torch_geometric的使用，一般用于图神经网络；
7. torch.manual_seed，设定随机数种子，多次运行程序时，可固定网络初始化；
8. 单机多卡训练,让模型在全部GPU上训练
```Python
net = XXXNet()
net = nn.DataParallel(net)
```
9. torch.bmm函数：计算矩阵乘法，两个矩阵的维度必须为3，即：shape_a = \[a, b, c], shape_b = \[a, c, d]
```Python
res = torch.bmm(ma, mb)
ma: [a, b, c]
mb: [a, c, d]
# 等价于
for i in range(a):
  ma[i] * mb[i]
# 结果维度
[a, b, d]
```
10. optimizer.zero_grad()，用在梯度计算和反向传播之前，将梯度置为0，防止受到前面batch的影响；
```
optimizer.zero_grad() # 清空之前的梯度
loss.backward() # 反向传播计算梯度
optimizer.step() # 更新梯度
```
11. 矩阵相乘
```
import torch
import numpy as np

a = torch.from_numpy(np.array([[1, 2], [3, 4]]))
b = torch.from_numpy(np.array([[1, 2], [3, 4]]))
# 点乘
r1 = a @ b
r2 = torch.matmul(a, b)
print(r1, '\n', r2)
# 按元素乘
r3 = a * b
r4 = torch.mul(a, b)
print(r3, '\n', r4)
```
