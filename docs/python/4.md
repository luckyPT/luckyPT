tensorflow
====
![api结构图](/docs/python/images/4-1.jpg)<br>
tensorflow依赖于一个很高效的C++后端进行计算，与后端通过一个session连接；通常情况是先创建一个图，然后在session中启动这个图。

### 基础概念 ###

如numpy一样，很多耗时的操作如：矩阵运算，都是在python环境的外部进行计算的，通过其他语言更高效的进行实现；但是这样会带来很多环境切换的成本。

对于TensorFlow来说进行了优化，TensorFlow也是在python环境外部完成其主要工作，但为了避免环境切换的开销，并没有将单个耗时的操作拿到python环境外部，而是先使用python描述一张交互图，然后整张图都是运行在python环境之外，与Theano或Torch的做法类似。因此python主要是用来构建图，并且指定运行图的哪一部分。

**变量**

① 占位符：不是一个特定的值，一般是在运行时输入；<br>
```x = tf.placeholder("float", [None, 784]) 表示一个二维浮点数张量，None表示该维度可以是任意长度```

② 变量：存在于图中的，运行过程中可修改的张量；通常模型的参数都使用变量表示；<br>
```W = tf.Variable(tf.zeros([784,10])) 其中 tf.zeros([784,10]) 表示变量的初值```

### api ###

softmax层：```y = tf.nn.softmax(tf.matmul(x,W) + b)```

交叉熵计算：```cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 其中y_表示真实值，y表示预测值```

梯度下降优化器：```train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)```

变量初始化：图在运行之前，或者是变量在使用之前，需要初始化变量：```init = tf.initialize_all_variables(); 但是此处并不是真正的初始化，真正的初始化是需要对init进行计算，即执行sess.run(init)```

张量在某个维度上的最大值的索引：```tf.argmax```

判断是否相等：```tf.equal```

强制转换：```tf.cast(correct_prediction, "float")``` 其中correct_prediction的元素类型为true和false，强制转为float

求和、求平均值：```tf.reduce_sum tf.reduce_mean```

Session&InteractiveSession：后者允许在进行图的时候插入一些计算图，这在交互式环境中十分方便；而前者必须要在启动图之前，构建好要运行的图。

正太分布初始化：```initial=tf.truncated_normal(shape, stddev=0.1)  tf.Variable(initial)``` 可以指定均值和方差

卷积处理：```tf.nn.conv1d  tf.nn.conv2d```

relu激活：```tf.nn.relu(input)``` 

**广播机制**

> if __name__ == '__main__':<br>
    tensor = tf.Variable(tf.ones(shape=[3, 3]), dtype=tf.float32)<br>
    one = tf.Variable(1, dtype=tf.float32)<br>
    with tf.Session() as sess:<br>
        sess.run(tf.global_variables_initializer())<br>
        print(sess.run(tensor + one))<br>
        print(sess.run(tf.nn.sigmoid(tensor)))<br>
  输出：<br>
  \[\[2. 2. 2.\]<br>
  \[2. 2. 2.\]<br>
  \[2. 2. 2.\]\]<br>
 \[\[0.7310586 0.7310586 0.7310586\]<br>
  \[0.7310586 0.7310586 0.7310586\]<br>
  \[0.7310586 0.7310586 0.7310586\]\]<br>
  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-<br>
  if __name__ == '__main__':<br>
    tensor = tf.Variable(tf.truncated_normal(shape=[3, 3]), dtype=tf.float32)<br>
    one_h = tf.Variable([[1, 2, 3]], dtype=tf.float32)<br>
    one_v = tf.Variable([[1], [2], [3]], dtype=tf.float32)<br>
    with tf.Session() as sess:<br>
        sess.run(tf.global_variables_initializer())<br>
        print(tensor.shape, sess.run(tensor))<br>
        print(one_h.shape, sess.run(one_h))<br>
        print(one_v.shape, sess.run(one_v))<br>
        print(sess.run(tensor + one_h))<br>
        print(sess.run(tensor + one_v))<br>
   输出：<br>
   (3, 3) \[\[ 0.16290209  0.01440278 -1.5861359 \]<br>
 \[-0.13474119 -1.3237085   0.22272775\]<br>
 \[ 0.46570358 -0.7127815  -0.8469924 \]\]<br>
(1, 3) \[\[1. 2. 3.\]\]<br>
(3, 1) \[\[1.\]<br>
 \[2.\]<br>
 \[3.\]\]<br>
\[\[1.1629021  2.0144029  1.4138641 \]<br>
 \[0.8652588  0.67629147 3.2227278 \]<br>
 \[1.4657036  1.2872186  2.1530075 \]\]<br>
\[\[ 1.1629021   1.0144027  -0.58613586\]<br>
 \[ 1.8652588   0.67629147  2.2227278 \]<br>
 \[ 3.4657035   2.2872186   2.1530075 \]\]<br>
 三行三列 + 一行三列 => 按行相加<br>
 三行三列 + 三行一列 => 按列相加<br>

转换数组形状：```tf.reshape```

** dropout层 ** <br>
将输入的tensor中的每个元素按照一定的概率置为0，如果不置为0，则按照一定一定的比例进行缩放，目的是为了保证和不变；keep_prob=0.2表示将80%的元素置为0剩下的20% 元素除以0.2<br>
dropout层的主要目的是为了防止过拟合。

tensor元素数量：```tf.size  b = tf.Variable(np.ones([10, 2])) #size=12```

增加维度：```tf.expand_dims``` 维度的计数由外往内，如\[\[1,2\],\[3,4\]\],第一个维度元素应该是\[1,2\]和\[3,4\]，第二维是1、2、3、4；
>   b = tf.Variable([[1, 2], [3, 4]])<br>
    c = tf.expand_dims(b, 0) # \[\[\[1,2],\[3,4\]\]\] 第二个参数不可以省略<br>
    \[\[\[1, 2\]\],\[\[3, 4\]\]\]

### 使用demo ###
