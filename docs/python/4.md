tensorflow
====
![api结构图](/docs/python/images/4-1.jpg)<br>
tensorflow依赖于一个很高效的C++后端进行计算，与后端通过一个session连接；通常情况是先创建一个图，然后在session中启动这个图。

### 基础概念 ###

如numpy一样，很多耗时的操作如：矩阵运算，都是在python环境的外部进行计算的，通过其他语言更高效的进行实现；但是这样会带来很多环境切换的成本。

对于TensorFlow来说进行了优化，TensorFlow也是在python环境外部完成其主要工作，但为了避免环境切换的开销，并没有将单个耗时的操作拿到python环境外部，而是先使用python描述一张交互图，然后整张图都是运行在python环境之外，与Theano或Torch的做法类似。因此python主要是用来构建图，并且指定运行图的哪一部分。

**变量**

① 占位符：不是一个特定的值，一般是在运行时输入；<br>
```x = tf.placeholder("float", [None, 784]) 表示一个二维浮点数张量，None表示该维度可以是任意长度```

② 变量：存在于图中的，运行过程中可修改的张量；通常模型的参数都使用变量表示；<br>
```W = tf.Variable(tf.zeros([784,10])) 其中 tf.zeros([784,10]) 表示变量的初值```

### api ###

查看sess中所有node：``` for node in sess.graph_def.node```

softmax层：```y = tf.nn.softmax(tf.matmul(x,W) + b)```

交叉熵计算：```cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 其中y_表示真实值，y表示预测值```

乘法计算：``` tf.multiply（）两个矩阵中对应元素各自相乘；tf.matmul（）将矩阵a乘以矩阵b，生成a * b。 ```

梯度下降优化器：```train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)```

变量初始化：图在运行之前，或者是变量在使用之前，需要初始化变量：```init = tf.initialize_all_variables(); 但是此处并不是真正的初始化，真正的初始化是需要对init进行计算，即执行sess.run(init)```

张量在某个维度上的最大值的索引：```tf.argmax```

判断是否相等：```tf.equal```

Tensor 获取shape：features.get_shape()

强制转换：```tf.cast(correct_prediction, "float")``` 其中correct_prediction的元素类型为true和false，强制转为float

求和、求平均值：```tf.reduce_sum tf.reduce_mean```

Session&InteractiveSession：后者允许在进行图的时候插入一些计算图，这在交互式环境中十分方便；而前者必须要在启动图之前，构建好要运行的图。

正太分布初始化：```initial=tf.truncated_normal(shape, stddev=0.1)  tf.Variable(initial)``` 可以指定均值和方差

卷积处理：```tf.nn.conv1d  tf.nn.conv2d```

relu激活：```tf.nn.relu(input)``` 

tensor反转：
```Python
tf.enable_eager_execution()
x = tf.constant(np.array([[1, 2, 3], [4, 5, 6]]))
print(tf.reverse(x, [0]))  # [[4 5 6],[1 2 3]]
print(tf.reverse(x, [1]))  # [[3 2 1],[6 5 4]]
print(tf.reverse(x, [0, 1]))  # [[6 5 4],[3 2 1]]
```

tensor map：
```Python
tf.enable_eager_execution()


def map_fun(x):
    print(x)
    return x


elems = tf.constant([[[1, 2], [3, 4]], [[4, 5], [6, 7]], [[7, 8], [9, 10]]])
# 将第一个维度的元素逐个进行map，（[[1, 2], [3, 4]]）（[[4, 5], [6, 7]]）（[[7, 8], [9, 10]]）
after_map = tf.map_fn(fn=lambda x: map_fun(x), elems=elems, dtype=tf.int32)
print(after_map)

elems = tf.constant([[b'1530/6', b'6133/3', b'10069/5', b'2131/4', b'9054/6', b'11158/6']])
alternates = tf.string_to_number(
    tf.map_fn(
        lambda x: tf.sparse_tensor_to_dense(tf.string_split(tf.reshape(x, [1]), delimiter='/'), default_value='0'),
        tf.reshape(elems, [-1, ]),
        dtype=tf.string)
    , out_type=tf.int32)
print(tf.reshape(alternates, [6, 2]))
```
tensor padding：
```Python
import tensorflow as tf

tf.enable_eager_execution()

a = tf.constant([1, 2, 3, 4])
a_pad = tf.pad(a, tf.constant([[3, 2]]))  # 一维tensor 左右填充数目
print(a_pad)

a = tf.constant([[1, 2], [3, 4]])
a_pad = tf.pad(a, tf.constant([[2, 1], [3, 4]]))  # 二维tensor 上下左右填充数目
print(a_pad)
```
tf.string_split

```Python
import tensorflow as tf

tf.enable_eager_execution()

string = tf.constant(["i am a student", 'very good', "you are right"])
string = tf.sparse_tensor_to_dense(tf.string_split(string), default_value="-")
print(string)

输出：tf.Tensor(
[[b'i' b'am' b'a' b'student']
 [b'very' b'good' b'-' b'-']
 [b'you' b'are' b'right' b'-']], shape=(3, 4), dtype=string)
```

**多分类问题**<br>
模型的输出是在各个类别上的概率分布，这时候求解损失函数一般有两种思路：<br>
1.将真实的结果映射为一个向量，向量维度就是类别数目，值非0即1；然后求交叉熵：```cross_entropy = -tf.reduce_sum(y_ * tf.log(y))```<br>
2.调用tf自带的损失函数：``` tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds) ```<br>

**广播机制**
```Python
> if __name__ == '__main__':<br>
    tensor = tf.Variable(tf.ones(shape=[3, 3]), dtype=tf.float32)<br>
    one = tf.Variable(1, dtype=tf.float32)<br>
    with tf.Session() as sess:<br>
        sess.run(tf.global_variables_initializer())<br>
        print(sess.run(tensor + one))<br>
        print(sess.run(tf.nn.sigmoid(tensor)))<br>
  输出：<br>
  [[2. 2. 2.]
  [2. 2. 2.]
  [2. 2. 2.]]
 [[0.7310586 0.7310586 0.7310586]
  [0.7310586 0.7310586 0.7310586]
  [0.7310586 0.7310586 0.7310586]]

  if __name__ == '__main__':<br>
    tensor = tf.Variable(tf.truncated_normal(shape=[3, 3]), dtype=tf.float32)<br>
    one_h = tf.Variable([[1, 2, 3]], dtype=tf.float32)<br>
    one_v = tf.Variable([[1], [2], [3]], dtype=tf.float32)<br>
    with tf.Session() as sess:<br>
        sess.run(tf.global_variables_initializer())<br>
        print(tensor.shape, sess.run(tensor))<br>
        print(one_h.shape, sess.run(one_h))<br>
        print(one_v.shape, sess.run(one_v))<br>
        print(sess.run(tensor + one_h))<br>
        print(sess.run(tensor + one_v))<br>
   输出：<br>
   (3, 3) [[ 0.16290209  0.01440278 -1.5861359 ]
 [-0.13474119 -1.3237085   0.22272775]
 [ 0.46570358 -0.7127815  -0.8469924 ]]
(1, 3) [[1. 2. 3.]]<br>
(3, 1) [[1.]<br>
 [2.]<br>
 [3.]]<br>
[[1.1629021  2.0144029  1.4138641 ]<br>
 [0.8652588  0.67629147 3.2227278 ]<br>
 [1.4657036  1.2872186  2.1530075 ]]<br>
[[ 1.1629021   1.0144027  -0.58613586]<br>
 [ 1.8652588   0.67629147  2.2227278 ]<br>
 [ 3.4657035   2.2872186   2.1530075 ]]<br>
 三行三列 + 一行三列 => 按行相加<br>
 三行三列 + 三行一列 => 按列相加<br>
```
转换数组形状：```tf.reshape```

**dropout层** <br>
将输入的tensor中的每个元素按照一定的概率置为0，如果不置为0，则按照一定一定的比例进行缩放，目的是为了保证和不变；keep_prob=0.2表示将80%的元素置为0剩下的20% 元素除以0.2<br>
dropout层的主要目的是为了防止过拟合。

tensor元素数量：```tf.size  b = tf.Variable(np.ones([10, 2])) #size=12```

扩展维度：```tf.expand_dims``` 维度的计数由外往内，如\[\[1,2\],\[3,4\]\],第一个维度元素应该是\[1,2\]和\[3,4\]，第二维是1、2、3、4；
>   b = tf.Variable([[1, 2], [3, 4]])<br>
    c = tf.expand_dims(b, 0) # \[\[\[1,2],\[3,4\]\]\] 第二个参数不可以省略<br>
    c = tf.expand_dims(b, 1) #\[\[\[1, 2\]\],\[\[3, 4\]\]\]
    
range函数： ```tf.range(0, 10, 2) # [0 2 4 6 8] shape (5,)```

矩阵链接：```tf.concat```
```Python
    a = tf.Variable([[1, 2], [3, 4]])<br>
    b = tf.Variable([[5, 6]])<br>
    c = tf.concat([a, b], axis=0)  # 保证列相同，直接追加<br>
    d = tf.concat([a, tf.reshape(b, shape=[2, 1])], axis=1)  # 保证第一维度的数量相等，将两者第一维度里面的元素拼接<br>
```
    
**数据输入**<br>
数据由外部输入到计算图中的几种方式：<br>
1)tf常量

2)通过占位符实现外部数据的输入

3)通过基于队列的输入通道<br>
根据文件名list创建文件名队列 -> tensorflow 完成由文件名队列到内存队列的映射 -> 启动

demo：
```Python
# coding:utf-8
import tensorflow as tf

file_list = ['../data/iris/setosa.txt', '../data/iris/versicolor.txt', '../data/iris/virginica.txt']

with tf.Session() as sess:
    # 根据文件名列表，生成文件名队列
    filename_queue = tf.train.string_input_producer(file_list, shuffle=False, num_epochs=1)
    # 创建读取器 reader
    reader = tf.TextLineReader()  # WholeFileReader()
    _, value = reader.read(filename_queue)
    # 初始化
    sess.run((tf.local_variables_initializer(), tf.global_variables_initializer()))
    # 启动
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess, coord)
    i = 0
    while i < 10:
        i += 1
        data = sess.run(value)
        print(data)
    coord.request_stop()
    coord.join(threads)
```

4)基于tf.data API的输入通道

### 使用demo ###

处理带有字符串的文本文件：<br>
文件格式：<br>
>"1",5.1,3.5,1.4,0.2,"setosa"<br>
"2",4.9,3,1.4,0.2,"setosa"<br>
"3",4.7,3.2,1.3,0.2,"setosa"<br>
...<br>

```Python
# coding:utf-8
import tensorflow as tf

file_list = ['../data/iris/setosa.txt', '../data/iris/versicolor.txt', '../data/iris/virginica.txt']

with tf.Session() as sess:
    # 根据文件名列表，生成文件名队列
    filename_queue = tf.train.string_input_producer(file_list, shuffle=False, num_epochs=1)
    # 创建读取器 reader
    reader = tf.TextLineReader()  # WholeFileReader()
    _, value = reader.read(filename_queue)
    data = tf.reshape(tf.decode_compressed(value), [1])
    cols = tf.sparse_tensor_to_dense(tf.string_split(data, ','), '0')
    sample_id = cols[:, 0:1]
    features = cols[:, 1:5]
    labels = cols[:, 5:]
    # 初始化
    sess.run((tf.local_variables_initializer(), tf.global_variables_initializer()))
    # 启动
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess, coord)
    i = 0
    while i < 2:
        print(sess.run((sample_id, features, labels)))
        i += 1
    # 终止程序
    coord.request_stop()
    coord.join(threads)

```

从文件创建dataset：<br>
```Python
# coding:utf-8
import tensorflow as tf

file_list = tf.constant(['../data/iris/setosa.txt', '../data/iris/versicolor.txt', '../data/iris/virginica.txt'])
dataset = tf.data.TextLineDataset(file_list).map(
    lambda line: tf.sparse_tensor_to_dense(tf.string_split(tf.reshape(line, [1]), ','), '0')).batch(10)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()
with tf.Session() as sess:
    sess.run((tf.global_variables_initializer(), tf.local_variables_initializer(), iterator.initializer))
    batch = sess.run(next_element)
    print(batch)
```

pipeline迭代数据<br>
文件格式：
>id,label,features<br>
6815395545287393491,0,0.0,0.6,0.4,0.6,0.4,0.2,0.6,0.2,0.8,0.0,0.6,0.0,0.0,0.4,0.0,0.4,0.6,0.4,0.4,0.875,1.0,0.368421,1.0,1.0,0.003893,0.509641,0.824324,0.8,0.0,0.8,1.0,0.358048,0.999994,0.174311,0.238,0.068,0.383,-0.019,0.141,0.257,-0.117,-0.037,0.2,0.178,-0.157,0.373,0.253,-0.123,-0.159,0.328,-0.181,-0.251,0.213,-0.073,0.043,0.008,-0.044,-0.143,-0.189,-0.019,0.295,-0.28,-0.037,0.098,-0.097,-0.047,-0.078,0.133,-0.282,0.242,0.261,0.235,-0.453,-0.164,0.167,0.165,-0.393,0.025,-0.272,-0.366,0.221,-0.22,-0.314,-0.002,-0.145,0.01,-0.092,-0.064,-0.01,0.111,-0.063,0.309,-0.11,-0.065,-0.062,0.114,-0.017,-0.033,0.276,0.073,-0.043,0.239,-0.145,-0.235,0.186,0.138,-0.039,0.267,0.279,-0.267,0.199,-0.162,-0.256,0.173,0.264,0.028,0.008,0.58,0.025,-0.102,-0.099,-0.012,0.254,-0.373,-0.484,-0.208,0.345,-0.248,-0.017,0.036,-0.255,0.269,-0.082,0.177<b
-7245910683568856490,0,0.0,1.0,0.8,0.4,0.8,0.2,0.6,0.2,0.2,0.0,0.2,0.4,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.875,0.5,0.210526,0.695652,0.5,0.017582,0.509641,0.0,0.2,0.0,0.25,0.0,0.0,0.0,0.106007,0.28,0.034,0.162,-0.324,0.212,0.119,-0.294,0.161,-0.097,-0.022,0.002,0.104,-0.054,-0.106,-0.173,0.299,0.081,-0.042,0.162,-0.204,0.067,0.11,-0.016,-0.13,-0.007,-0.288,0.222,-0.154,-0.025,0.213,-0.095,-0.166,-0.282,-0.196,-0.179,0.044,0.13,-0.234,-0.153,-0.18,0.233,0.01,-0.078,0.029,0.085,-0.305,0.116,-0.324,-0.088,0.128,-0.183,-0.169,-0.284,0.085,-0.34,-0.073,0.046,0.231,0.042,0.049,-0.012,-0.212,0.156,0.031,0.343,-0.02,-0.286,0.265,0.071,0.085,-0.365,-0.008,-0.017,0.205,0.17,-0.395,0.173,-0.231,-0.128,0.455,0.197,-0.127,0.236,0.195,0.166,0.243,0.185,-0.041,0.098,-0.022,-0.263,-0.134,0.045,-0.137,0.059,0.062,-0.358,0.279,-0.352,0.262<br>
4283854114731685190,1,0.0,0.4,0.0,0.8,0.0,0.2,0.6,0.2,0.4,0.0,0.2,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.125,0.5,1.0,0.826086,1.0,0.189592,0.509641,0.0,0.7,0.0,0.725,1.0,0.237594,0.999987,0.384614,0.141,0.0,0.29,-0.343,0.259,0.164,-0.11,0.091,0.091,0.166,-0.111,0.107,-0.21,0.018,-0.205,0.239,-0.277,-0.117,0.192,-0.288,0.09,-0.035,0.099,-0.077,-0.107,-0.111,0.212,-0.123,0.106,0.261,-0.139,-0.096,-0.125,-0.252,-0.406,0.049,0.093,-0.058,-0.128,-0.236,0.335,0.015,-0.237,0.051,0.167,-0.078,0.129,-0.31,-0.26,0.226,-0.189,-0.22,-0.33,-0.035,-0.246,0.094,-0.005,0.205,-0.138,0.066,-0.199,0.002,-0.021,0.046,0.214,-0.123,0.139,0.142,-0.155,0.093,-0.451,-0.068,-0.04,0.083,0.025,-0.038,0.189,-0.284,-0.298,0.311,0.048,-0.022,0.161,0.184,0.045,-0.25,0.273,-0.019,0.158,-0.121,-0.184,-0.13,0.198,-0.16,-0.002,0.067,-0.35,0.266,-0.35,-0.03<br>

```Pythpn
# coding:utf-8
import tensorflow as tf

tf.app.flags.DEFINE_string('dataPath', '../data/info_flow/*', 'dataPath')
tf.app.flags.DEFINE_integer('batch_size', 4, 'dataPath')
tf.app.flags.DEFINE_integer('prefetch_size', 1024, 'dataPath')
FLAGS = tf.app.flags.FLAGS


def parse_fn(line):
    feature_label = tf.string_to_number(
        tf.sparse_tensor_to_dense(tf.string_split(tf.reshape(line, [1]), ','), '0')[:, 1:], tf.float32)
    feature = feature_label[:, 2:]
    label = feature_label[:, 1:2]
    return feature, label


def input_fn():
    files = tf.data.Dataset.list_files(FLAGS.dataPath)
    dataset = files.interleave(tf.data.TextLineDataset, cycle_length=1).map(map_func=parse_fn, num_parallel_calls=1)
    dataset = dataset.batch(batch_size=FLAGS.batch_size)
    dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_size)
    return dataset


ds = input_fn()
iterator = ds.make_one_shot_iterator()
next_element = iterator.get_next()
with tf.Session() as sess:
    sess.run((tf.global_variables_initializer(), tf.local_variables_initializer()))
    print(sess.run(next_element))

```
dataset与estimator配合使用(数据同上)<br>
```Python
# coding:utf-8
import tensorflow as tf

tf.app.flags.DEFINE_string('train_data_path', '../data/info_flow/test-001', 'trainDataPath')
tf.app.flags.DEFINE_string('test_data_path', '../data/info_flow/test-002', 'testDataPath')
tf.app.flags.DEFINE_integer('num_parallel_readers', 8, 'num_parallel_readers')
tf.app.flags.DEFINE_integer('batch_size', 1024, 'dataPath')
tf.app.flags.DEFINE_integer('prefetch_size', 10240, 'dataPath')
FLAGS = tf.app.flags.FLAGS
tf.logging.set_verbosity(tf.logging.INFO)

# 1个label，134个feature
COLUMNS = [str(i) for i in range(0, 135)]
FILE_DEFAULT = [0 for i in range(0, 135)]


def tensor_scalar(t):
    return tf.reshape(t, [])


def parse_fn(line):
    feature_label = tf.reshape(tf.string_to_number(
        tf.sparse_tensor_to_dense(tf.string_split(tf.reshape(line, [1]), ','), '0')[:, 1:],
        tf.float32),
        [135, ])
    columns_value = map(tensor_scalar, tf.split(feature_label, 135))
    features = dict(zip(COLUMNS, columns_value))
    label = features.pop('0')
    return features, label


def input_fn(data_path, repeat_count):
    files = tf.data.Dataset.list_files(data_path)
    # dataset = files.interleave(tf.data.TextLineDataset, cycle_length=1) \
    dataset = files.apply(tf.contrib.data.parallel_interleave(
        tf.data.TextLineDataset, cycle_length=FLAGS.num_parallel_readers)) \
        .repeat(repeat_count) \
        .shuffle(buffer_size=10000)
    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_fn, batch_size=FLAGS.batch_size))
    # dataset = dataset.map(map_func=parse_fn, num_parallel_calls=1).batch(batch_size=FLAGS.batch_size)
    dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_size)
    return dataset.make_one_shot_iterator().get_next()


checkpoint_config = tf.estimator.RunConfig(
    save_checkpoints_secs=10 * 60,
    keep_checkpoint_max=10
)
feature_columns = [tf.feature_column.numeric_column(str(i)) for i in range(1, 135)]
est = tf.estimator.DNNClassifier(hidden_units=[1000, 1000, 1000, 1000], feature_columns=feature_columns, n_classes=2,
                                 model_dir="../model",
                                 config=checkpoint_config)
est.train(input_fn=lambda: input_fn(FLAGS.train_data_path, 1000), steps=1000)
eval_result = est.evaluate(input_fn=lambda: input_fn(FLAGS.train_data_path, 1))
print(eval_result)


```
look up(simple demo)<br>
```Python
import tensorflow as tf

'''
首先生成矩阵，行数为离散变量所有可能取值的数量，列数为embedding之后向量维度
训练时需要首先将离散变量从0开始依次映射为id，通过lookup就可以方便的返回矩阵的第i行
之后就可以正常训练，并且在训练过程中会更新embedding矩阵中的值
'''
vocabulary_size = 1
embedding_size = 4
embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
data = tf.placeholder(tf.int32, shape=[1])
label = tf.placeholder(tf.int32, shape=[1])
embed = tf.reshape(tf.nn.embedding_lookup(embeddings, data), [4, 1])
weight = tf.Variable(initial_value=tf.random_normal(shape=[1, embedding_size]))
output = tf.matmul(weight, embed)
loss = tf.losses.mean_squared_error(label, tf.reshape(output, shape=[1, ]))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)
with tf.Session() as sess:
    sess.run((tf.global_variables_initializer(), tf.local_variables_initializer()))
    for i in range(0, 100):
        print(sess.run((optimizer, output, embeddings, weight), feed_dict={data: [0], label: [10]}))

```

tensorflow 求函数导数
```Python
from __future__ import absolute_import, division, print_function

import tensorflow as tf
import tensorflow.contrib.eager as tfe

tf.enable_eager_execution()


def square(x):
    if x > 0:
        return tf.multiply(x, x)
    else:
        return tf.multiply(-1, tf.multiply(x, x))


# 计算一阶导数
grad = tfe.gradients_function(square)
# 计算二阶导数
gradgrad = tfe.gradients_function(lambda x: grad(x)[0])

print(square(3.))  # [9.]
print(grad(3.)[0])  # [6.]
print(gradgrad(3.)[0])  # [2.]

print(square(-3.))  # [9.]
print(grad(-3.)[0])  # [6.]
print(gradgrad(-3.)[0])  # [2.]

```
