集成决策树：随机森林&GBDT&XGBOOST
====
#### 随机森林 ####


#### GBDT ####


#### XGBOOST ####
从损失函数说起，对于传统的算法来说损失函数一般包含两部分：一部分用来衡量真实值与预测值之间的误差称为训练损失，另一部分用来衡量模型的复杂程度作为惩罚项成为正则项；

通用的表示方式如下：```obj(θ)=L(θ)+Ω(θ)``` 其中L(θ) 表示训练损失，Ω(θ)表示正则项。

xgboost采用CART树作为最基分类器，CART树与其他分类树不同的是，每个叶子节点对应一个打分值；多棵树可以组合为一个分类器，然后将每棵树的分值相加作为最终的score：
>![模型打分](/docs/ml/images/8-1.jpg)  <br>其中fk(xi) 表示第k棵树对样本点xi的打分; 
对应损失函数，更一般化的表示如下：<br>![损失函数](/docs/ml/images/8-2.jpg)<br>
第一项表示训练损失，第二项表示模型复杂度

***训练过程***

如何得到每一棵决策树呢？大致思路是使用递增的策略，固定已经学到的预测值，然后训练新的一棵树拟合真实值与已经学到的值的误差（也可以称为残差）；每一棵树对样本点的预测值如下：<br>![训练过程](/docs/ml/images/8-3.jpg)

损失函数变形如下<br>
![损失函数](/docs/ml/images/8-4.jpg)

泰勒级数如下：<br>
![泰勒级数](/docs/ml/images/8-5.jpg)

根据泰勒级数，对损失函数进行变形，如下：<br>
![损失函数](/docs/ml/images/8-6.jpg)
>说明：将第t棵树的预测值看作自变量，将训练损失看作因变量；


