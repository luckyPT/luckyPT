集成决策树：随机森林&GBDT&XGBOOST
====
#### 随机森林 ####


#### GBDT ####


#### XGBOOST ####
从损失函数说起，对于传统的算法来说损失函数一般包含两部分：一部分用来衡量真实值与预测值之间的误差称为训练损失，另一部分用来衡量模型的复杂程度作为惩罚项成为正则项；

通用的表示方式如下：```obj(θ)=L(θ)+Ω(θ)``` 其中L(θ) 表示训练损失，Ω(θ)表示正则项。

xgboost采用CART树作为最基分类器，CART树与其他分类树不同的是，每个叶子节点对应一个打分值；多棵树可以组合为一个分类器，然后将每棵树的分值相加作为最终的score：
>![模型打分](/docs/ml/images/8-1.jpg)  <br>其中fk(xi) 表示第k棵树对样本点xi的打分; 
对应损失函数，更一般化的表示如下：<br>![损失函数](/docs/ml/images/8-2.jpg)<br>
第一项表示训练损失，第二项表示模型复杂度

***训练过程***

如何得到每一棵决策树呢？大致思路是使用递增的策略，固定已经学到的预测值，然后训练新的一棵树拟合真实值与已经学到的值的误差（也可以称为残差）；每一棵树对样本点的预测值如下：<br>![训练过程](/docs/ml/images/8-3.jpg)

损失函数变形如下<br>
![损失函数](/docs/ml/images/8-4.jpg)

泰勒级数如下：<br>
![泰勒级数](/docs/ml/images/8-5.jpg)

根据泰勒级数，对损失函数进行变形，如下：<br>
![损失函数](/docs/ml/images/8-6.jpg)
>说明：将第t棵树的输出值以及之前树的输出值的求和作为预测值，将这个预测值看作自变量，将第t棵树的输出值作为增量，将训练损失看作因变量；训练损失是预测值的函数，而每棵树的输出值是预测值的变化量。

去除常量，对损失函数进行化简：<br>
![损失函数](/docs/ml/images/8-7.jpg)
>训练第t棵树是，前面的t-1棵树已经确定，所以l(yi,ŷ (t−1)i) 是常量，并且ŷ (t−1)i的值是确定的，而l(yi,ŷi)函数是确定的，所以可以求函数在ŷ (t−1)i处的一阶导数和二阶导数(如果存在)

***正则项***

正则项定义如下：<br>
![正则项](/docs/ml/images/8-8.jpg)
>说明：T表示叶子节点的个数，w表示叶节点对应的权重；γ 和λ为参数

对损失函数进一步化简:<br>
![损失函数](/docs/ml/images/8-9.jpg)
>说明：上述化简合并是基于一棵树的输出值就是叶子节点的分数，所以输出值是有限个，对应的分数w也是有限个，对应的一阶导数和二阶导数也是有限个；同一个叶子节点的同类项进行了合并化简。

进一步化简<br>
![损失函数](/docs/ml/images/8-10.jpg)<br>
![损失函数](/docs/ml/images/8-11.jpg)
> 上面的公式类似数学上的二次函数求最值。w相当于x，Gj和0.5(Hj+λ)相当于系数，注意 这里的w为一棵树的预测值，这个值并不是根据叶子所包含的样本点的值进行平均得到的；w可以有很多取值，根据损失函数，应该取使得损失函数最小的值。
