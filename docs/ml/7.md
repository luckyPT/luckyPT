集成学习算法
====
集成学习的核心思想是多个弱分类器组合成强分类器。下面先介绍一个简单的多分类器组成强分类的例子<br>
> 假如有三个弱分类器进行某项二分类任务的正确率都是0.6；现在组合一个分类器，组合后的分类器按照投票结果，取票数最多的结果作为最终结果；那么这个分类器的正确率为：三个都分类正确的概率 + 两个分类正确的概率 = 0.6^3 + 3 * 0.6 * 0.6 * 0.4 = 0.648

那么什么样的弱分类器可以组合成强分类器呢？ 一句话概括就是“好而不同”的基分类器；也就是说基分类器最好是满足两个特点：效果好、彼此之间差异大（或者说是彼此之间独立性高）<br>
集成学习算法可以分为两类，Bagging和Boosting，下面详细介绍

#### Boosting ####
boosting算法的思想是根据前面个分类器的效果去调整数据集的分布，使得后面的分类器可以弥补前分类器的不足；<br>
boosting的一般描述如下：
![boosting算法描述](/docs/ml/images/7-1.jpg)<br>

***adaboost*** 对boosting算法做了一些改进，算法实现流程如下：<br>
![adaboost算法描述](/docs/ml/images/7-2.jpg)<br>

具体计算实例：
![adaboost计算实例](/docs/ml/images/7-3.jpg)<br>
其中Zm是为了保证权重（Wm）之和为1；
αm就是第m个分类器的权重；
如果标签为-1 和 +1最终的预测表达式为：<br>
![预测表达式](/docs/ml/images/7-5.jpg)  关于sign(x)  当x>0，sign(x)=1;当x=0，sign(x)=0; 当x<0， sign(x)=-1；<br>
如果标签为0和1,最终表达式为：


关于adaboost的合理性，也是有严谨的数学证明的<br>
![adaboost数学证明](/docs/ml/images/7-4.jpg)<br>

#### baging ####
