Seq2Seq
====
### 最简单的描述
Seq2Seq的简单描述就是将一段Seq编码为定长的向量，然后这个定长的向量再解码为一段Seq；如下图：<br>
![Seq2Seq的简单描述](/docs/ml/images/12_1-1.jpg)

### 关于编码器和解码器的工作原理
Seq2Seq模型由两部分构成，一部分是编码器，另一部分是解码器；

编码器的作用就是将Seq编码为定长的向量，一般通过RNN来实现，与一般模型中的RNN层没有区别；可以将最后时刻的神经元状态作为编码向量。

解码器的作用是将定长向量解码称为一段Seq，仍然通过RNN层来实现，与一般模型RNN层的区别是，在预测的时候，初始化状态是编码向量，第一个时刻输入是<START>或者是编码器的输出（如果由于维度原因不能将编码器输出作为解码器输入，则可以通过全连接转换即可），下一时刻的输入是上一时刻的输出的预测值。
  
训练的时候对于解码器来说，除了将编码向量作为初始化状态之外，第一时刻的输入是<START>或者编码器的输出，下一时刻的输入应该是上一时刻输出的真实值（如果是预测阶段就是预测值）。

### 最基础的Seq2Seq网络结构
最简单的基于RNN的Seq2Seq的网络结构如下：<br>
![Seq2Seq的基础模型](/docs/ml/images/12_1-2.jpg)

### LSTM、多层、多时刻的变种
Seq2Seq有一些变种，比如对于RNN的选择，可以选最基础的RNN，也可以选择LSTM、GRU等<br>
同时也可以是单层或者多层；基于多层RNN的Seq2Seq模型如下图：<br>
![多层RNN](/docs/ml/images/12_1-3.jpg)

基于LSTM的Seq2Seq模型如下：<br>
![基于LSTM的Seq2Seq](/docs/ml/images/12_1-5.jpg)

另外的变种做法，将最终的编码向量用于每一时刻的输入，而不是用于初始化状态；此类网络的结构如下图:<br>
![Seq2Seq的简单描述](/docs/ml/images/12_1-4.jpg)

### 加入Attention机制的变种
Attention机制是基于上面提到的，编码向量用于每一时刻的的输入进行变种而来的。<br>
Attention机制将编码器的每一步隐藏状态进行连接，得到一个”语义向量“~~（设置：return_sequences=True即可）~~；<br>
~~这个向量在解码的每一步使用不同的权重做一次全连接，然后将全连接之后的输出作为这一时刻的输入，这种做法虽然有效果，但并不是经典的Attention机制。~~

正确的attention值是根据解码时（前一个/或第一个）时刻的隐藏状态跟编码每一个时刻的隐藏状态计算dot，然后进行softmax得到seq_length个α值；<br>
每一个时刻都会得到seq_length个α值

有两种类型的attention机制，一种是global，另一种是local （参考论文：Effective Approaches to Attention-based Neural Machine Translation.pdf）


global类型的attention 网络结构如下图：<br>
![加入Attention机制的Seq2Seq](/docs/ml/images/12_1-6.jpg)<br>

官方实现：https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention

### 实现
以上各种Seq2Seq的实现：[Seq2Seq](https://github.com/luckyPT/py_ml/blob/master/src/tf/Seq2Seq/BaseSeq2Seq.py)

