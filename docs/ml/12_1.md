Seq2Seq
====
### 最简单的描述
Seq2Seq的简单描述就是将一段Seq编码为定长的向量，然后这个定长的向量再解码为一段Seq；如下图：<br>
![Seq2Seq的简单描述](/docs/ml/images/12_1-1.jpg)

### 关于编码器和解码器的工作原理
Seq2Seq模型由两部分构成，一部分是编码器，另一部分是解码器；

编码器的作用就是将Seq编码为定长的向量，一般通过RNN来实现，与一般模型中的RNN层没有区别；可以将最后时刻的神经元状态作为编码向量。

解码器的作用是将定长向量解码称为一段Seq，仍然通过RNN层来实现，与一般模型RNN层的区别是，在预测的时候，初始化状态是编码向量，第一个时刻输入是<START>或者是编码器的输出（如果由于维度原因不能将编码器输出作为解码器输入，则可以通过全连接转换即可），下一时刻的输入是上一时刻的输出的预测值。
  
训练的时候对于解码器来说，除了将编码向量作为初始化状态之外，第一时刻的输入是<START>或者编码器的输出，下一时刻的输入应该是上一时刻输出的真实值（如果是预测阶段就是预测值）。

### 最基础的Seq2Seq网络结构
最简单的基于RNN的Seq2Seq的网络结构如下：<br>
![Seq2Seq的基础模型](/docs/ml/images/12_1-2.jpg)

### LSTM、多层、多时刻的变种
Seq2Seq有一些变种，比如对于RNN的选择，可以选最基础的RNN，也可以选择LSTM、GRU等<br>
同时也可以是单层或者多层；基于多层RNN的Seq2Seq模型如下图：<br>
![多层RNN](/docs/ml/images/12_1-3.jpg)

基于LSTM的Seq2Seq模型如下：<br>
![基于LSTM的Seq2Seq](/docs/ml/images/12_1-5.jpg)

另外的变种做法，将最终的编码向量用于每一时刻的输入，而不是用于初始化状态；此类网络的结构如下图:<br>
![Seq2Seq的简单描述](/docs/ml/images/12_1-4.jpg)

### 加入Attention机制的变种


### 实现
