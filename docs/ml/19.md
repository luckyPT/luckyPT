Transformer
====
### 简介
本质上依然是Encoder-Decoder架构，只不过在当中引入了：位置编码、self-attention、多头注意力、paddingMask和lookaheadMask以及经典的残差连接和归一化等，概念比较多，所以无论是理论还是实现都略微复杂，但并不是很难；

### 位置编码及应用
位置编码就是每一个位置信息的表示，计算方式如下：<br>
PE(pos,2i) = sin(pos/pow(10000,2i/d)) <br>
PE(pos,2i+1) = cos(pos/pow(10000,2i/d)) <br>
这里的pos指的是字符(或词语)在seq中的位置，这里的i是维度等于d_model的一个向量，d是d_model的维度；<br>
也就是在位置向量的偶数位置是通过正弦求得的，奇数位置是通过余弦求得。<br>
特别说明的是，在第0和第1个位置，2i均为0；第2个和第3个位置，2i均为2；依次类推...<br>
[代码](https://github.com/luckyPT/py_ml/blob/master/src/tf/transformer/position_encoder.py)<br>

下面以seq = 3；d_model = 5,为例，介绍位置编码的求解；<br>
首先计算角度：<br>
```Python
pos=np.arange(3)[:, np.newaxis] #array([[0],[1],[2]])
i=np.arange(5)[np.newaxis, :] #array([[0, 1, 2, 3, 4]]) 这里是维度为d_model的向量，也是pos_embedding的原型 
#计算角度 angle的shape为3*5，其中3是由于pos导致的，5是由于i的维度导致的
angle=pos/np.power(10000,2*(i//2)/5) # 2*(i//2)的结果是[[0,0,2,2,4]] 很有规律
#拆分奇偶位置，并计算正余弦
sin=np.sin(angle[:,0::2])
cos=np.cos(angle[:,1::2])
#可能是考虑到embedding内部的位置对算法效果没有影响，因此再合并sin和cos的时候，并没有间隔插入合并，而是直接合并的
pos_encoding = np.concatenate([sin, cos], axis=-1)
pos_encoding = pos_encoding[np.newaxis, ...]
```
这里最终每一个位置的向量维度与词语embeding的向量维度相同，两者按元素相加即可得出这个词在这个位置的向量表示；
### 注意力模型
这里理解起来会有一定难度，网上的各种介绍几乎也都是千篇一律的“官话”；闲话少叙，来看看以下介绍是不是可以让你更明白：
首先我们要理解注意力模型是在解决什么类型的问题，主要是解决在已经有一定的信息知识的条件下，判断某种情况对应的输出结果；<br>
这里涉及三个变量：Q、K、V<br>
K - 表示已经有的信息知识；<br>
Q - 也就是上面提到的某种情况；<br>
Q和K通过计算会得到一组权重，表示在Q这种情况下，K中各种信息的影响力；用这个权重与V进行元素对应相乘（可能会涉及到广播）就是模型的输出；<br>
这里要求K的序列长度和V的序列长度相同，从物理意义上来讲对应位置的元素应该是有非常强的联系的；一般在模型中，K和V是相同的；

下面举一个例子：
“我、是、程、序、员”这五个字可以表示为5×128维的向量，这个向量作为K和V；假设将“我” 作为查询向量Q（维度是1×128），首先计算权重，跟每一个词做点积会得到一个数值，最终得到1×5的一个向量，然后进行softmax 得到这5个词语对于“我”这种情况的一个权重（1×5），然后与V进行相乘，得到一个1×128维向量，这个向量可以表示“我” 在 “我是程序员” 这个句子的某些含义，比如：词性是“主语”

#### transformer中的self attention
Attention(Q,K,V)=softmax((Q\*K<sup>T</sup>)/sqrt(d))\*V 
对于encoder来说，Q、K、V就是postion embedding和word embedding的求和得到的向量（暂且记为input），通过一层全连接得到；Q=W<sub>q</sub>\*input  K=W<sub>k</sub>\*input  V=W<sub>v</sub>\*input<br>
这里除以sqrt(d)的目的是为了梯度稳定，随着dmodel的增大，Q\*K<sup>T</sup>的值也会增大；可以通过求导证明，对于softmax函数来说，变量x越大，导数值越小，
对于decoder来说，？？

### 残差连接与归一化

### 编码器层

### 解码器层

### 整体架构
