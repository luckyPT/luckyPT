Transformer
====
### 简介
本质上依然是Encoder-Decoder架构，只不过在当中引入了：位置编码、self-attention、多头注意力、paddingMask和lookaheadMask以及经典的残差连接和归一化等，概念比较多，所以无论是理论还是实现都略微复杂，但并不是很难；

### 位置编码及应用
位置编码就是每一个位置信息的表示，计算方式如下：<br>

这里最终每一个位置的向量维度与词语embeding的向量维度相同，两者按元素相加即可得出这个词在这个位置的向量表示；
### 注意力模型
这里理解起来会有一定难度，网上的各种介绍几乎也都是千篇一律的“官话”；闲话少叙，来看看以下介绍是不是可以让你更明白：
首先我们要理解注意力模型是在解决什么类型的问题，主要是解决在已经有一定的信息知识的条件下，判断某种情况对应的输出结果；<br>
这里涉及三个变量：Q、K、V<br>
K - 表示已经有的信息知识；<br>
Q - 也就是上面提到的某种情况；<br>
Q和K通过计算会得到一组权重，表示在Q这种情况下，K中各种信息的影响力；用这个权重与V进行元素对应相乘（可能会涉及到广播）就是模型的输出；<br>
这里要求K的序列长度和V的序列长度相同，从物理意义上来讲对应位置的元素应该是有非常强的联系的；一般在模型中，K和V是相同的；

下面举一个例子：
“我、是、程、序、员”这五个字可以表示为5×128维的向量，这个向量作为K和V；假设将“我” 作为查询向量Q（维度是1×128），首先计算权重，跟每一个词做点积会得到一个数值，最终得到1×5的一个向量，然后进行softmax 得到这5个词语对于“我”这种情况的一个权重（1×5），然后与V进行相乘，得到一个1×128维向量，这个向量可以表示“我” 在 “我是程序员” 这个句子的某些含义，比如：词性是“主语”




### 残差连接与归一化

### 编码器层

### 解码器层

### 整体架构
