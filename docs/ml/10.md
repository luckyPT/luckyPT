常见激活函数&优化器
====

### 激活函数 ###
激活函数主要作用是在神经网络模型中引入非线性特性；对于激活函数来说，常常关注以下特性：

① 是否有梯度消失或者梯度爆炸问题<br>
② 函数值是否对称
③ 是否单侧抑制

常见的激活函数：<br>

** relu **
![relu激活函数](/docs/ml/images/10-1.jpg)<br>
特性：单侧抑制，所以可以使得神经元具有稀疏激活性，即当输入值为负值时输出为0，导数也为0；可以更好地挖掘特征；<br>
使用RELU激活函数时，一般要求偏置不为0；避免出现0梯度。

### 优化器 ###
