常见损失函数&激活函数&优化器
====
### 损失函数 ###
损失函数用来计算和衡量预测值与真实值之间的误差的函数，一般可以将预测值看作是变量。

1.均方差
> Σ(y-f(x))<sup>2</sup>  <br>
求解方便，但对于异常值和离群值比较敏感

2.交叉熵
>-Σy<sub>i</sub> log(y<sub>i</sub>^)

3.相对熵

4.0-1损失

5.绝对值损失
>相比均方差，绝对值更加稳健，对异常值不敏感，鲁棒性好。但在极值点处存在越变，不利于学习。

6.指数损失

### 激活函数 ###
激活函数主要作用是在神经网络模型中引入非线性特性；对于激活函数来说，常常关注以下特性：

① 是否有梯度消失或者梯度爆炸问题<br>
② 函数值是否对称<br>
③ 是否单侧抑制

常见的激活函数：

**sigmod**

![sigmod激活函数](/docs/ml/images/10-2.jpg)<br>
![sigmod激活函数](/docs/ml/images/10-3.jpg)<br>
特性：输出值介于0~1之间，可以被表示概率。具有软饱和性，容易产生梯度消失的问题。（对于梯度消失问题，除了换激活函数之外，也有类似于DBN的分层预训练、Batch Normalization的逐层归一化，Xavier和MSRA权重初始化等方式）；收敛比较慢，深度网络中一般应用较少。

**relu**

![relu激活函数](/docs/ml/images/10-1.jpg)<br>
特性：单侧抑制，所以可以使得神经元具有稀疏激活性，即当输入值为负值时输出为0，导数也为0；可以更好地挖掘特征；<br>
使用RELU激活函数时常常使用一个较小的正数来初始化偏置项，避免输出为0（死亡神经元）。<br>
左侧硬饱和激活函数。

### 优化器 ###
