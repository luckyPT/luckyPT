线性回归
====

## 数学原理 ##

在[1.个人对机器学习的理解中](/docs/ml/1.md)中提到过，机器学习就是利用数学函数对数据集进行拟合；而数学函数的类型一般都是预先定义好的，求解过程实际上是在求解函数的参数；

线性回归算法是假设输入变量与输出变量之间存在着线性关系，其数学函数形式为```y=k*x+b```,特别说明一下，这里的k和x可能是数字，也可能是向量；y一般是一个数字；

任何算法的求解都是从损失函数开始入手的，因为损失函数的最优值是算法求解的目标；线性回归的损失函数一般定义为：
> cost = (1/2m) ∑(y^-y)^2 <br>
其中 y^ 表示算法的预测值，y表示算法的真实值；求这两个数之差的平方，作为一个样本点的损失，然后各个样本点再求和，再求平均值；至于又乘了1/2是为了后面求导的方便；这个损失函数称为 均方差（MSE mean square error）损失函数

下面说明怎么由损失函数入手，求解k和b的最优值！

![线性回归数学推导](/docs/ml/images/2-1.jpg)

## 关于正则化 ##

正则化是用来防止过拟合的一种手段，具体到线性回归来说，有两种正则化方式，分别是L1正则化和L2正则化；
正则化的具体实现是对损失函数做一定的修改：
> L1正则化的损失函数：cost = (1/2m) ∑(y^-y)^2 + ∑|w|   <br> 
L2正则化的损失函数：cost = (1/2m) ∑(y^-y)^2 + ∑w^2 <br>


正则化的数学原理如下：
![正则化数学原理](/docs/ml/images/2-2.jpg)

正则化为什么可以防止过拟合，一般有两点解释：
- 由于正则化可以限制权重W的大小，从某种意义上可以降低模型复杂度，可以具有更好泛化能力。另外，奥卡剃姆刀原理也说明模型复杂度越小越好
- 正则化可以认为是一个惩罚项，只有当每次更新获得更大的收益时，才会选择更新；这样有利于减少噪声的影响。
