循环神经网络
====
循环神经网络特点是可以挖掘出数据序列之间的关系信息，实际使用中每一个样本的输入是数据序列，也就是一系列的数据，其中的每个数据是一个时间步。

### RNN
RNN层也是由一个或者多个神经元组成的，每个神经元的输入由两部分构成，一部分是序列数据中的某一个数据，另一部分是这个数据的前一个数据经过循环层神经元时，神经元输出的隐藏状态。神经元的输出也包含两部分，一部分时输出的预测值，另一部分时隐藏状态。RNN的结构图如下：<br>
![RNN神经元](/docs/ml/images/12-6.jpg)<br>
其网络结构与计算过程可从如下Demo中得知：<br>
```Python
# coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

tf.enable_eager_execution()
x = np.linspace(-100, 100, 1000)
y = np.sin(x)
time_step = 50


class MinimalRNNCell(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        self.units = units
        self.state_size = units
        super(MinimalRNNCell, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                      initializer='uniform',
                                      name='kernel')
        self.recurrent_kernel = self.add_weight(
            shape=(self.units, self.units),
            initializer='uniform',
            name='recurrent_kernel')
        self.built = True

    def call(self, inputs, states):
        prev_output = states[0]
        h = tf.matmul(inputs, self.kernel)
        output = h + tf.matmul(prev_output, self.recurrent_kernel)
        return output, [output]


def to_train_label(serial, time_step=50):
    inputs = []
    out = []
    for i in range(serial.shape[0] - time_step - 1):
        inputs.append(serial[i:i + time_step])
        out.append(serial[i + time_step])

    return np.array(inputs), np.array(out)


input_data, label = to_train_label(y, time_step)
input_data = tf.expand_dims(input_data, 2)
label = tf.expand_dims(label, 1)
print(input_data.shape, label.shape)
input = tf.keras.Input(shape=[time_step, 1])
cells = MinimalRNNCell(64)

rnn = tf.keras.layers.RNN(cells)(input)
out = tf.keras.layers.Dense(units=1)(rnn)

model = tf.keras.Model(inputs=input, outputs=out)
model.compile(optimizer=tf.train.AdamOptimizer(), loss='mean_squared_error')
model.fit(input_data, label, batch_size=512, epochs=100)

start = np.expand_dims(input_data.numpy()[-1], 0).copy()
pres = []
for i in range(50):
    next = model.predict(start)
    start[0, :time_step - 1] = start[0, 1:]
    start[0, time_step - 1] = next
    pres.append(next[0][0])

last = np.ndarray.flatten(input_data.numpy()[-1])

all = np.concatenate((last, pres))
plt.plot(list(range(len(all))), all)
plt.show()
print(all)

```


自定义RNN神经元：https://keras.io/zh/layers/recurrent/

### LSTM
下面以LSTM层中只有一个神经元为例(units=1)，说明前向传播过程。下面的o<sub>t</sub>,h<sub>t</sub>,c<sub>t</sub>都是一维的。如果units不只一个，则每个神经元均按照如下方式计算，可类比一个全连接层有一个和多个神经元。同一层的这些神经元之间是没有联系的。

前向传播：<br>
![LSTM神经元](/docs/ml/images/12-1.jpg)
输入：本次输入X(t)，神经元的上一个状态C(t-1),神经元的上一个隐藏状态H(t-1)<br>
输出：本次更新后的神经元状态C(t),本次的隐藏状态H(t)<br>
计算详述：<br>
遗忘门计算：<br>
![遗忘门](/docs/ml/images/12-2.jpg)<br>

输入门与本次计算状态：<br>
![输入门与状态梯度](/docs/ml/images/12-3.jpg)<br>

状态更新计算：<br>
![状态更新计算](/docs/ml/images/12-4.jpg)<br>

输出门与输出计算：<br>
![输出门与输出计算](/docs/ml/images/12-5.jpg)<br>

**其他变种**

参考文献：http://colah.github.io/posts/2015-08-Understanding-LSTMs/

### Bi-LSTM

### DEMO
使用keras的循环层的时候，如果循环层是最外层，那么循环层的input shape的第一个维度为seq_length。如果不是最外层，第一个维度为序列中每个数据的shape
```Python
# coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

tf.enable_eager_execution()
x = np.linspace(-100, 100, 1000)
y = np.sin(x)
time_step = 50


def to_train_label(serial, time_step=50):
    inputs = []
    out = []
    for i in range(serial.shape[0] - time_step - 1):
        inputs.append(serial[i:i + time_step])
        out.append(serial[i + time_step])

    return np.array(inputs), np.array(out)


input_data, label = to_train_label(y, time_step)
input_data = tf.expand_dims(input_data, 2)
label = tf.expand_dims(label, 1)
print(input_data.shape, label.shape)
input = tf.keras.Input(shape=[time_step, 1])
rnn = tf.keras.layers.LSTM(units=128, input_shape=[1, ])(input)
out = tf.keras.layers.Dense(units=1)(rnn)

model = tf.keras.Model(inputs=input, outputs=out)
model.compile(optimizer=tf.train.AdamOptimizer(), loss='mean_squared_error')
model.fit(input_data, label, batch_size=512, epochs=50)

model.save("../model/my.h5")
start = np.expand_dims(input_data.numpy()[-1], 0).copy()
pres = []
for i in range(50):
    next = model.predict(start)
    start[0, :time_step - 1] = start[0, 1:]
    start[0, time_step - 1] = next
    pres.append(next[0][0])

last = np.ndarray.flatten(input_data.numpy()[-1])

all = np.concatenate((last, pres))
plt.plot(list(range(len(all))), all)
plt.show()
print(all)
```
### 应用
