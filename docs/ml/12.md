循环神经网络
====
循环神经网络特点是可以挖掘出数据序列之间的关系信息，实际使用中每一个样本的输入是数据序列，也就是一系列的数据，其中的每个数据是一个时间步。

### RNN
自定义RNN神经元：https://keras.io/zh/layers/recurrent/

### LSTM
前向传播：<br>
![LSTM神经元](/docs/ml/images/12-1.jpg)
输入：本次输入X(t)，神经元的上一个状态C(t-1),神经元的上一个隐藏状态H(t-1)<br>
输出：本次更新后的神经元状态C(t),本次的隐藏状态H(t)<br>
计算详述：<br>
遗忘门计算：<br>
![遗忘门](/docs/ml/images/12-2.jpg)<br>

输入门与本次计算状态：<br>
![输入门与状态梯度](/docs/ml/images/12-3.jpg)<br>

状态更新计算：<br>
![状态更新计算](/docs/ml/images/12-4.jpg)<br>

输出门与输出计算：<br>
![输出门与输出计算](/docs/ml/images/12-5.jpg)<br>

**其他变种**


参考文献：http://colah.github.io/posts/2015-08-Understanding-LSTMs/
### DEMO
使用keras的循环层的时候，如果循环层是最外层，那么循环层的input shape的第一个维度为seq_length。如果不是最外层，第一个维度为序列中每个数据的shape
```Python
# coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

tf.enable_eager_execution()
x = np.linspace(-100, 100, 1000)
y = np.sin(x)
time_step = 50


def to_train_label(serial, time_step=50):
    inputs = []
    out = []
    for i in range(serial.shape[0] - time_step - 1):
        inputs.append(serial[i:i + time_step])
        out.append(serial[i + time_step])

    return np.array(inputs), np.array(out)


input_data, label = to_train_label(y, time_step)
input_data = tf.expand_dims(input_data, 2)
label = tf.expand_dims(label, 1)
print(input_data.shape, label.shape)
input = tf.keras.Input(shape=[time_step, 1])
rnn = tf.keras.layers.LSTM(units=128, input_shape=[1, ])(input)
out = tf.keras.layers.Dense(units=1)(rnn)

model = tf.keras.Model(inputs=input, outputs=out)
model.compile(optimizer=tf.train.AdamOptimizer(), loss='mean_squared_error')
model.fit(input_data, label, batch_size=512, epochs=50)

model.save("../model/my.h5")
start = np.expand_dims(input_data.numpy()[-1], 0).copy()
pres = []
for i in range(50):
    next = model.predict(start)
    start[0, :time_step - 1] = start[0, 1:]
    start[0, time_step - 1] = next
    pres.append(next[0][0])

last = np.ndarray.flatten(input_data.numpy()[-1])

all = np.concatenate((last, pres))
plt.plot(list(range(len(all))), all)
plt.show()
print(all)
```
### 应用
