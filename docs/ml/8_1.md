支持向量机（SVM）
====

支持向量机一般是用来处理二分类问题，可以通过一些改进处理多分类和回归问题，核心是找出**“最大距离”**的分界线，所谓的最大距离指的是两个分类到分界线的距离和最大；分类到分界线的距离取这个分类样本点到分解线距离的最小值；

支持向量机的原型是线性分类器，对于线性不可分的问题是采用一定的容忍度或者借助核函数去解决；

对于一条直线或者一个超平面，可以用```y=wx+b```表示；SVM分类器就是在寻找这样一个超平面，用于分割两个分类。当```wx+b>0```时，属于其中一类，```wx+b<0```时，属于另一类。

### 线性可分 ###
对于逻辑回归来说，也是在寻找一条直线或者是一个超平面，LR和SVM在预测形式上是相同的，二者的区别在于损失函数与求解方式不同。
![逻辑回归详解](/docs/ml/2.md)<br>
SVM的损失函数是带有约束的优化问题，如下图：<br>
![SVM损失函数](/docs/ml/images/8_1-1.jpg)<br>

**拉格朗日乘数法**<br>
拉格朗日乘数法是一种寻找变量受一个或多个条件所限制的情况下多元函数的极值的方法。这种方法将一个有n 个变量与k 个约束条件的最优化问题转换为一个有n + k个变量的方程组的极值问题，其变量不受任何约束。举例：<br>
![拉格朗日乘数法实例](/docs/ml/images/8_1-2.jpg)<br>
很显然，满足对λ求导为0的点，自然就满足了约束条件;以上是带等式约束条件的最优问题转化，对于带不等式的，则需要满足KKT(三个人名的首字母)条件，则可以使用广义拉格朗日乘数法进行转换：<br>

使用拉格朗日乘数法，将上述带约束的损失函数转为不带约束的损失函数，如下：<br>



**原始问题与对偶问题**<br>




### 线性不可分 ###


参考文献：<br>
https://charlesliuyx.github.io/2017/09/20/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%B3%95%E5%92%8CKKT%E6%9D%A1%E4%BB%B6/

https://blog.csdn.net/yujianmin1990/article/details/48494607
