支持向量机（SVM）
====

支持向量机一般是用来处理二分类问题，可以通过一些改进处理多分类和回归问题，核心是找出**“最大距离”**的分界线，所谓的最大距离指的是两个分类到分界线的距离和最大；分类到分界线的距离取这个分类样本点到分解线距离的最小值；

支持向量机的原型是线性分类器，对于线性不可分的问题是采用一定的容忍度或者借助核函数去解决；

对于一条直线或者一个超平面，可以用```y=wx+b```表示；SVM分类器就是在寻找这样一个超平面，用于分割两个分类。当```wx+b>0```时，属于其中一类，```wx+b<0```时，属于另一类。

### 线性可分 ###
对于逻辑回归来说，也是在寻找一条直线或者是一个超平面，LR和SVM在预测形式上是相同的，二者的区别在于损失函数与求解方式不同。
[逻辑回归详解](/docs/ml/2.md)<br>
SVM的损失函数是带有约束的优化问题，如下图：<br>
![SVM损失函数](/docs/ml/images/8_1-1.jpg)<br>
约束条件也可写为```y(wx+b)>1```

**拉格朗日乘数法**<br>
拉格朗日乘数法是一种寻找变量受一个或多个条件所限制的情况下多元函数的极值的方法。这种方法将一个有n 个变量与k 个约束条件的最优化问题转换为一个有n + k个变量的方程组的极值问题，其变量不受任何约束。举例：<br>
![拉格朗日乘数法实例](/docs/ml/images/8_1-2.jpg)<br>
很显然，满足对λ求导为0的点，自然就满足了约束条件;<br>
以上是带等式约束条件的最优问题转化，对于带不等式的，满足KKT(三个人名的首字母)条件下，可以使用广义拉格朗日乘数法进行转换：<br>
？？？广义推导暂时略过？？？<br>

上述带约束的损失函数转为不带约束的损失函数，并进行化简 如下：<br>
![转化后的损失函数](/docs/ml/images/8_1-3.jpg)<br>

整个过程，实际上是让损失函数关于alpha求最大化，然后让损失函数关于w b求最小化的过程。

**原始问题与对偶问题**<br>


### 线性不可分 ###

#### 软间隔 ####

#### 核函数 ####
对于线性不可分的问题，映射到高维也许就可分了；通过前面的数学推导，可以看出在求解过程中只需要用到样本特征内积。

那么我们假设可以将X映射到高维，在计算过程中就需要计算高维特征下的内积。很多时候并不确定映射到多少维合适，所以很难直接将维映射到合适高维。而且特征稍微一多就容易造成维度灾难。

但我们只要将低维度的内积映射到高维度的内积就可以，也就是值到值的映射，即二维空间下的点，在有限样本点下，应该可以找到一条曲线去拟合这些值到值的映射。

核函数就是将低维度特征的内积映射为高维度的内积的函数。


参考文献：<br>
https://blog.csdn.net/v_july_v/article/details/7624837

https://charlesliuyx.github.io/2017/09/20/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%B3%95%E5%92%8CKKT%E6%9D%A1%E4%BB%B6/

https://blog.csdn.net/yujianmin1990/article/details/48494607

https://blog.csdn.net/stdcoutzyx/article/details/9774135

https://www.zhihu.com/question/36694952

https://www.cnblogs.com/liqizhou/archive/2012/05/11/2495689.html
