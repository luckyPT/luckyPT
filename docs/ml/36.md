行为检测综述
====
## 背景
预测主车周边的物体（行人、自行车、其他车辆等）在未来一段时间的运行轨迹，对于主车的规划、决策是至关重要的；<br>
考虑到目前端到端的自动驾驶还不能支撑量产需求，在量产车落地上，还是将自动驾驶划分为：感知、规划、决策三个主要阶段，其中行为预测属于感知的一个子任务；<br>
对上承接感知层面的一些输出，如：物体的类型、bound box、历史轨迹、速度、加速度、角速度等；<br>
结合地图信息、车道线、红绿灯等<br>
输出每一个物体在未来一小段时间内（通常是10秒左右）的运动轨迹；<br>

## 任务
利用感知信息融合地图信息，预测物体未来一段时间的轨迹（坐标）；<br>
输入：<br>
- 感知信息：物体类型、bound box、位置、速度、角速度、加速度、红绿灯状态、周边其他物体信息、车道线信息
- 地图信息：周边建筑物信息（如：停车场）、车道线信息等

输出：<br>
- 物体未来10秒左右的运动轨迹

## 解决方案
### 运动学模型
加速度对时间的积分是速度变化，速度对时间的积分是位移；

### 意图模型
利用交通规则等信息，识别物体的意图，进而预测其轨迹，比如：遇见红绿灯要停车，拐弯要减速等；

### 交互模型
考虑主车与周围环境的交互，比如：识别其他物体的意图（比如：判断旁边车辆是否要变道等），根据其他物体的意图来预测主车轨迹；

早期的模型都是基于规则生成的轨迹预测逻辑；

### 深度学习（RNN模型）

### 基于GAN的行为预测（S-GAN）

### PECNet & R-PECNe

### Trajectron++*

### vectornet


## 评价指标
### ADE
所有时刻的位移误差，求平均值；（求平均值时，分母为：物体数量 × 时刻数量）

### FDE
只看最终时刻的位移误差，求平均值；（分母为：物体数量）

## 开源数据集
### ETH数据集
https://paperswithcode.com/dataset/eth<br>
采用鸟瞰视角采集的固定位置的视频数据，视频中内容为行人运动，没有车辆；<br>
**单应性矩阵：** 解决同一物理世界对应不同视角图像的坐标系之间的变换的一个矩阵；<br>
预测方案：https://github.com/HarshayuGirase/Human-Path-Prediction

### UCY数据集

### SDD数据集

### Argoverse dataset

## 论文解读
[From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting](https://arxiv.org/pdf/2012.01526.pdf)<br>
基于目标、历史轨迹、道路对行人轨迹进行长期预测；<br>
给定一个场景以及物体在这个场景下过去5秒的运动轨迹，预测未来1分钟内可能的一个或者多个运行轨迹<br>
**问题分析** <br>
通常路径预测会从两个重要依据出发：<br>
1. 基于认知理解的预测；（比如：分析行人的目的地，然后预测路径）；<br>
2. 基于历史轨迹和道路信息预测所有可能的走法<br>
论文中提出了Y-NET网络，可以平衡这两种信息依据；<br>

即使在知道目标的前提下，也不一定能够准确预测轨迹，仍然需要考虑很多的环境因素，比如：是否等红绿灯，是否避让他人，绿灯是否即将变红灯等，甚至人的目标也有可能是随时变换的，所以即使环境相同，但人的目标有变化后，轨迹依然会有所不同；

所以面临的任务，无论是对行人目标的意图理解，还是外部环境都是有不确定性的；

**建模思路** <br>
首先根据场景图（RGB图像）与用户的历史轨迹，对轨迹的终点分布情况进行预测（也就是1分钟后大概会在什么位置）；这一阶段主要解决认知不确定性<br>
然后基于场景图情况，结合上面预测的终点，随机选取可能的路径点；这一阶段解决受环境影响的不确定性<br>

**过去相关工作** <br>
模型输入：历史轨迹、场景图、行人姿势、分割后语义线索、位置信息、其他行人信息、车辆信息、信号灯、目的地；<br>
分类：从行人视角结合姿势信息等预测固定的轨迹、利用生成式模型思路生成模型轨迹、数字概率估计(比如：估计到达每一个像素/网格的概率)<br>
**Y-NET网络结构** <br>
![网络结构](/docs/ml/images/Y-NET网络简介.png)
**损失函数** <br>
BCEWithLogitsLoss = Sigmoid + BCELoss；<br>
论文中损失函数有三部分，在Y-net中，损失函数有两部分，加权求和，先说标签：<br>
标签1：目标点，会用get_patch函数转为一张图；跟Ug解码的输出计算损失<br>
标签2：轨迹点，会用get_patch函数转为一张图；跟Ut解码的输出计算损失<br>

[VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation](https://arxiv.org/pdf/2005.04259.pdf)<br>
**创新点** <br>
1. 相较于之前都是用图片来表征场景和轨迹，该论文提出用向量来表示；
2. 增加了一个辅助任务训练模型

**向量化** <br>
向量化之前的信息表述形式：<br>
HD map中的组件表述：几何图形 + 属性信息；<br>
agent的轨迹：随时间变化的曲线 <br>

HD map组件向量化：等距离取一系列关键点，相邻关键点连接，构成一系列向量；<br>
轨迹向量化：从时间维度，隔一定时间（0.1秒）采样一个点，相邻的点连接，构成一系列的向量；<br>
多段线记为：P，多段线中的一个向量记为v，v = \[d_start, d_end, a, j] 其中d_start,d_end表示起止点，a表示附加属性，如：时间戳、红绿灯状态等、j表示多段线的id<br>
总结：无论是地图组件信息，还是轨迹信息，都转化为一系列的向量集合；（无序集合）<br>
一个细节，论文中在单个物体维度上，对点坐标进行了归一化，归一化操作选取的中心点是最后时刻物体的位置；(作者提到，在未来会跨物体进行归一化)<br>

**模型结构** <br>
![网络结构](/docs/ml/images/vector_net.png)

**代码解读** <br>
compute_feature_module.py文件用于读取argoverse的数据，并做初步处理，处理逻辑如下：<br>
agent_feature：历史运动轨迹（以可见的最后一刻坐标为中心，归一化处理）、agent类型、时间戳、轨迹id、未来运动轨迹；<br>
```Python
def get_agent_feature_ls(agent_df, obs_len, norm_center):
    """
    args:
    returns: 
        list of (doubeld_track, object_type, timetamp, track_id, not_doubled_groudtruth_feature_trajectory)
    """
    xys, gt_xys = agent_df[["X", "Y"]].values[:obs_len], agent_df[[
        "X", "Y"]].values[obs_len:]
    xys -= norm_center  # normalize to last observed timestamp point of agent
    gt_xys -= norm_center  # normalize to last observed timestamp point of agent
    xys = np.hstack((xys[:-1], xys[1:]))

    ts = agent_df['TIMESTAMP'].values[:obs_len]
    ts = (ts[:-1] + ts[1:]) / 2

    return [xys, agent_df['OBJECT_TYPE'].iloc[0], ts, agent_df['TRACK_ID'].iloc[0], gt_xys]
```

lane_feature_ls:获取最后一个可见时刻位置一定半径范围内的路段，并提取如下信息：是否交通管制、是否处于交汇处、路段中心线点坐标，维度（N,3）、基于中心线计算的左右边线坐标(有了左右边线，丢去了中心线信息)；<br>
```Python
lane_feature_ls.append([halluc_lane_1, halluc_lane_2, traffic_control, is_intersection, lane_id]);
```

obj_feature_ls：一定距离内，周边移动物体的信息，对于观测数据不足50个或者静止物体，进行了过滤，最终提取：轨迹、物体类型、时间戳、轨迹ID
```Python
obj_feature_ls.append([xys, remain_df['OBJECT_TYPE'].iloc[0], ts, track_id])
```

最终初步提取的信息为:\[agent_feature, obj_feature_ls, lane_feature_ls, norm_center]


## 参考资料
2014年论文：https://hal.inria.fr/hal-01053736/document 附论文解读：https://zhuanlan.zhihu.com/p/158951141<br>
2019年论文：https://arxiv.org/pdf/1912.11676.pdf 附论文解读：https://blog.csdn.net/weixin_35448535/article/details/112585397<br>
2020年，基于深度学习的方法综述：http://kzyjc.alljournals.cn/kzyjc/article/abstract/2020-1841<br>
2020年，https://arxiv.org/pdf/2012.01526.pdf<br>
行人检测+路径预测：https://github.com/YapingZ/Pedestrian-behavior-trajectory-prediction


