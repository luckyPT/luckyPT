决策树
====
通过之前对线性回归和逻辑回归的学习，应该了解到 损失函数是研究一个算法的着手点，也是模型优化的指标；下面介绍决策树的损失函数，需要从信息熵说起

#### 信息熵 ####
熵是对不确定性的测量，一个随机变量X的信息熵定义为：```H(X)=Σp(x)log p(x)``` 其中，x是X的一种可能结果；

决策树有很多种类型，常见的有ID3、C4.5、CART树，下面一一介绍

#### ID3 ####
决策树构建的关键是选择合适的特征值作为树节点的分裂标准，那么应该如何选择呢？<br>
对于ID3来说，是将信息增益作为分裂结果的衡量，信息增益就是分裂前的信息熵与分裂后的信息熵的差值，这个差值越大越好；分裂后的信息熵是计算分裂后的每一个集合的信息熵，然后按照样本比例加权求和（总体权重为1）<br>
基于这个标准，ID3只能处理离散特征和分类问题；对于回归问题和连续特征，则可能需要通过一定的方式转化为分类问题和离散特征去处理。

树的构建过程就是遍历离散特征进行分类，然后寻找最优特征，循环进行下去，直到节点不能再分裂或者是满足预剪枝的条件（预剪枝在后面会提到）

ID3算法的缺点：
- 使用ID3算法构建决策树的过程是一种贪心算法，也就是并不能保证整体最优；
- ID3选用的是信息增益作为衡量标准，这会偏重于那些包含种类较多的特征，因为从信息增益角度来说，显然是分的越细，信息增益往往就越大，但是在这样的分裂往往不具备较好的泛化能力。比如训练集中样本ID作为特征，显然是最好的特征，但是并不能具备很好的泛化能力

#### C4.5 ####
c4.5是对ID3算法的改进，主要是解决ID3在特征选择上偏重于分类值比较多的特征；<br>
c4.5与ID3的主要区别在于，特征选择的标准不同，C4.5的特征选择是根据信息增益率（信息增益比）作为标准，选择特征。
信息增益率是通过引入一个被称为分裂信息的惩罚项来惩罚取值较多的Feature，分裂信息用来衡量分裂的广度和均匀性，计算方式如下：
>![分裂信息计算公式](/docs/ml/images/6-1.jpg) <br>
其中D表示分裂前的集合，Di表示分裂后的其中一个子集，A表示分裂的属性<br>
信息增益率的计算公式如下：<br>
![信息增益率](/docs/ml/images/6-2.jpg)

C4.5虽然解决了ID3偏重于选择特征值较多的特征，但是也存在一些弊端，当Di的大小跟D比较接近时，分裂信息的值就会接近于0,这时候就会导致信息增益率较大。实际使用中的解决办法是，只对那些信息增益较大的分裂选择项（如增益高于平均增益的选择项），去计算信息增益率

另外，C4.5在具体实现上，解决了ID3不能使用连续特征的不足之处，解决方法是：
- 把需要处理的样本或者样本子集按照连续值的大小，从小到达进行排序
- 假设有N个不同的特征值，则会产生N-1个分裂点，分裂点的选择一般选相邻两个值的中间值
- 用信息增益率选择最佳划分，一般采用二分类器做做分割

第三点改进，可以对缺失值进行处理，针对缺失值，可以根据其他样本赋予默认值，如：取平均值，取出现次数最多的值，或者是一些加权计算的方法

但是C4.5仍然只能解决分类问题，这是由于特征选择的标准决定的

#### cart ####
CART树是一棵二叉树（id3和c4.5不是），是用的最多的一种树，既可以处理分类问题，也可以处理回归问题。

在处理分类问题和回归问题时，选择分裂的标准不同，处理分类问题时一般选择基尼系数作为分裂依据（主要是计算简单一些），在处理回归问题时一般选用均方差作为分裂依据。
>基尼系数的计算方式如下<br>
![基尼系数计算方式](/docs/ml/images/6-3.jpg)
