决策树
====
通过之前对线性回归和逻辑回归的学习，应该了解到 损失函数是研究一个算法的着手点，也是模型优化的指标；下面介绍决策树的损失函数，需要从信息熵说起

#### 信息熵 ####
熵是对不确定性的测量，一个随机变量X的信息熵定义为：```H(X)=Σp(x)log p(x)``` 其中，x是X的一种可能结果；

决策树有很多种类型，常见的有ID3、C4.5、CART树，下面一一介绍

#### ID3 ####
决策树构建的关键是选择合适的特征值作为树节点的分裂标准，那么应该如何选择呢？<br>
对于ID3来说，是将信息增益作为分裂结果的衡量，信息增益就是分裂前的信息熵与分裂后的信息熵的差值，这个差值越大越好；分裂后的信息熵是计算分裂后的每一个集合的信息熵，然后按照样本比例加权求和（总体权重为1）<br>
基于这个标准，ID3只能处理离散特征和分类问题；对于回归问题和连续特征，则可能需要通过一定的方式转化为分类问题和离散特征去处理。

树的构建过程就是遍历离散特征进行分类，然后寻找最优特征，循环进行下去，直到节点不能再分裂或者是满足预剪枝的条件（预剪枝在后面会提到）

ID3算法的缺点：
- 使用ID3算法构建决策树的过程是一种贪心算法，也就是并不能保证整体最优；
- ID3选用的是信息增益作为衡量标准，这会偏重于那些包含种类较多的特征，因为从信息增益角度来说，显然是分的越细，信息增益往往就越大，但是在这样的分裂往往不具备较好的泛化能力。比如训练集中样本ID作为特征，显然是最好的特征，但是并不能具备很好的泛化能力

#### C4.5 ####
c4.5是对ID3算法的改进，主要是解决ID3在特征选择上偏重于分类值比较多的特征；<br>
c4.5与ID3的主要区别在于，特征选择的标准不同，C4.5的特征选择是根据信息增益率（信息增益比）作为标准，选择特征。
信息增益率是通过引入一个被称为分裂信息的惩罚项来惩罚取值较多的Feature，分裂信息用来衡量分裂的广度和均匀性，计算方式如下：
>![分裂信息计算公式](/docs/ml/images/6-1.jpg) 其中D表示分裂前的集合，Di表示分裂后的其中一个子集，A表示分裂的属性


