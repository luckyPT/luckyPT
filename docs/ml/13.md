语言模型
====
### 词语的表示 ###
- onehot方式

词向量的维度为词汇表的大小，向量的每一位表示一个词；实际应用中容易带来维度灾难，另外也很难刻画词与词之间的相似程度。
- word vector方式

以固定大小，一般为100～300维的向量表示一个词，这里词向量是通过一定的方式训练出来的；这些词向量组成一个向量空间每一个词（词向量）是这个空间中的一个点，这样就可以引入空间距离来表示词的相似性；

具体来说，距离相近的词其空间距离应该也是相近的；词与词之间的差异性应该有一定的规律，或者说词与词之间构成的向量有相似性，比如中国与北京这两个点构成的向量应该和法国与巴黎构成的向量是一样或者近似的。

词向量是与语言模型捆绑在一起的，是语言模型训练结果的副产物；也就是通过神经网络模型训练的直接目的并不是得到词向量，而是为了做预测，比如一致某个词的上下文，预测这个词是什么（CBOW模型），或者是已知某个词，预测这个词的上下文等（skipgram），在这个训练过程中，产生了词向量。

### 语言模型 ###

语言模型就是为语句的联合概率函数进行建模，希望模型对有意义的语句赋予较大的概率，对没意义的语句赋予较小的概率；实际上就是语句的联合概率函数，下面介绍常见的集中语言模型。

- **n-gram模型**

n-gram模型是一种基于统计的语言模型，模型计算中所用参数都是基于已知语料统计来的。

假设由w1、w2、w3 ... wt组成的文本，其概率计算应该为 = ```P(w1)*P(w2|w1)*P(w3|w1w2)*...*P(wt|w1w2w3...wt-1)```

对于```P(wi|w1w2...wi-1)```的计算，等于```count(w1w2w3...wi)/count(w1w2w3...wi-1)``` ；

基于上述思想建模存在两个问题，数据稀疏严重；参数空间太大，无法实用

引入马尔可夫假设：一个词的出现，仅仅依赖于其前面的一个或者几个词；

1)假如不依赖于前面的词

概率为：```p(w1)*p(w2)*p(w3)*p(w4)...p(wt)```

2)假如依赖于前面1个词（2-gram/bigram）

概率为：```P(w1)*P(w2|w1)*P(w3|w2)*...*P(wt|wt-1)```

3)假如依赖于前面的2个词（3-gram/trigram）

概率为：```P(w1)*P(w2|w1)*P(w3|w1w2)*P(w4|w2w3)*...*P(wt|wt-2wt-1)```

实际使用中，为了避免数据溢出、提高性能，通常使用log后的加法运算代替乘法运算：<br>
```log(p(w1)*p(w2)*p(w3)*p(w4)...p(wt)) = log(p(w1)) + log(p(w2)) + log(p(w3)) + ... + log(p(wt))```

n-gram的应用：搜索自动提示、拼写错误检查、文本分类

- **神经概率语言模型**


- **cbow**

cbow模型是输入某个位置的上下文，输出各个词在这个位置的概率；常常使用神经网络模型；

- **skip-gram**

skip-gram模型是输入某个词，输出各个词在这个词上下文的概率；常常使用神经网络模型；

### word2vec ###
- 霍夫曼编码

从霍夫曼树说起，对于一堆给定的文本语料，如何构建霍夫曼树？首先是统计出每个词的词频，然后按照如下规则构建树：

选出词频最小的和次小的两个词，最小的作为右子树，次小的作为左子树，将两个词的词频之和作为根的词频；后面将这棵树视作一个词，词频就是根的词频；<br>
重复上述步骤。

示例：
![霍夫曼树的构建](/docs/ml/images/13-1.jpg)

霍夫曼编码：基于霍夫曼树，向右走记为1,向左走记为0；从根节点到每个词（叶节点）的唯一路径便产生出唯一编码，并且不同的词对应不同的编码；
我：1，喜欢：000，观看：001，巴西：010，足球：0110，世界杯：0111

“我喜欢观看巴西足球世界杯”的编码为100000101001100111
