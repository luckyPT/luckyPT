语言模型
====
### 词语的表示 ###
- onehot方式

词向量的维度为词汇表的大小，向量的每一位表示一个词；实际应用中容易带来维度灾难，另外也很难刻画词与词之间的相似程度。
- word vector方式

以固定大小，一般为100～300维的向量表示一个词，这里词向量是通过一定的方式训练出来的；这些词向量组成一个向量空间每一个词（词向量）是这个空间中的一个点，这样就可以引入空间距离来表示词的相似性；

具体来说，距离相近的词其空间距离应该也是相近的；词与词之间的差异性应该有一定的规律，或者说词与词之间构成的向量有相似性，比如中国与北京这两个点构成的向量应该和法国与巴黎构成的向量是一样或者近似的。

词向量是与语言模型捆绑在一起的，是语言模型训练结果的副产物；也就是通过神经网络模型训练的直接目的并不是得到词向量，而是为了做预测，比如一致某个词的上下文，预测这个词是什么（CBOW模型），或者是已知某个词，预测这个词的上下文等（skipgram），在这个训练过程中，产生了词向量。

### 语言模型 ###

语言模型就是为语句的联合概率函数进行建模，希望模型对有意义的语句赋予较大的概率，对没意义的语句赋予较小的概率；实际上就是语句的联合概率函数，下面介绍常见的集中语言模型。

- **n-gram模型**

n-gram模型是一种基于统计的语言模型，模型计算中所用参数都是基于已知语料统计来的。

假设由w1、w2、w3 ... wt组成的文本，其概率计算应该为 = ```P(w1)*P(w2|w1)*P(w3|w1w2)*...*P(wt|w1w2w3...wt-1)```

对于```P(wi|w1w2...wi-1)```的计算，等于```count(w1w2w3...wi)/count(w1w2w3...wi-1)``` ；

基于上述思想建模存在两个问题，数据稀疏严重；参数空间太大，无法实用

引入马尔可夫假设：一个词的出现，仅仅依赖于其前面的一个或者几个词；

1)假如不依赖于前面的词

概率为：```p(w1)*p(w2)*p(w3)*p(w4)...p(wt)```

2)假如依赖于前面1个词（2-gram/bigram）

概率为：```P(w1)*P(w2|w1)*P(w3|w2)*...*P(wt|wt-1)```

3)假如依赖于前面的2个词（3-gram/trigram）

概率为：```P(w1)*P(w2|w1)*P(w3|w1w2)*P(w4|w2w3)*...*P(wt|wt-2wt-1)```

实际使用中，为了避免数据溢出、提高性能，通常使用log后的加法运算代替乘法运算：<br>
```log(p(w1)*p(w2)*p(w3)*p(w4)...p(wt)) = log(p(w1)) + log(p(w2)) + log(p(w3)) + ... + log(p(wt))```

n-gram的应用：搜索自动提示、拼写错误检查、文本分类

如何解决平滑问题？？

1) 加k平滑<br>
![加1平滑](/docs/ml/images/13-5.jpg)<br>
当k=1时，称为加1平滑；V 为所有**可能出现的**，不同的n-gram的数量；相当于任何可能的组合出现次数至少为1次，保证所有可能的情况的计算概率为1；
2) Good-Turing平滑<br>
基本思想就是对于没看见的事件，从看见的事件中分出一小部分概率赋予这些没看见的事件。<br>
记r表示某个gram出现了r次，Nr表示一共有Nr个词出现了r次；则：<br>
![式13-6](/docs/ml/images/13-6.jpg)<br>
下面开始调整原有的计次：<br>
![式13-7](/docs/ml/images/13-7.jpg)<br>
也就是说，原先某一个n-gram对应计数r次，现在变为r\*次，如果有Nr个词对应计数为r次，那么这Nr个词的计数都变为r\*次<br>
对于Nr或者N(r+1) = 0的情况，可以通过插值或者其他方式，求Nr或者N(r+1)的期望E(Nr)或N(r+1)

3) 回退算法<br>
4) 插值算法<br>

参考文献：http://www.shuang0420.com/2017/03/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95(Smoothing)%E5%B0%8F%E7%BB%93/

https://heshenghuan.github.io/2016/05/13/Good-Turing%E4%BC%B0%E8%AE%A1/

https://ilewseu.github.io/2018/05/07/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/

- **神经概率语言模型**


- **cbow**

cbow模型是输入某个位置的上下文，输出各个词在这个位置的概率；常常使用神经网络模型；

基于Hierarchical Softmax的cbow模型

前向传播如下：<br>
1) 输入层<br>
w上下文的2c个词向量,前面c个词的词向量和后面c个词的词向量；这2c个词向量可以随机初始化，后面会通过反向传播更新。<br>
每一次的更新，同一个样本上下文的2c个词的更新梯度是一样的，但是由于不同词会出现在不同的训练样本中，所以会导致词与词之间的向量会产生差异。

2) 投影层<br>
将输入层2c个词向量做累加操作，得到Xw，这里并没有激活函数（后面反向传播会更新Xw，实际上是更新每一个输入的词向量，这里的更新导致每个词向量不同维度会产生差异）

3) 输出层<br>
输出层是一颗霍夫曼树，每一个非叶子节点对应一组权重，用于跟Xw做计算，得出向左走和向右走的概率；<br>
这样就可以计算出每个叶节点对应词的概率；
![CBOW 前向传播示例图](/docs/ml/images/13-2.jpg)

反向传播更新：<br>
![CBOW 反向传播更新参数](/docs/ml/images/13-3.jpg)<br>

模型伪代码<br>
![CBOW 反向传播更新参数](/docs/ml/images/13-4.jpg)


基于Negative Sampling的cbow模型

- **skip-gram**

skip-gram模型是输入某个词，输出各个词在这个词上下文的概率；常常使用神经网络模型；

基于Hierarchical Softmax的skip-gram模型

基于Negative Sampling的skip-gram模型

cbow模型和skip-gram模型区别：


### word2vec ###
- 霍夫曼编码

从霍夫曼树说起，对于一堆给定的文本语料，如何构建霍夫曼树？首先是统计出每个词的词频，然后按照如下规则构建树：

选出词频最小的和次小的两个词，最小的作为右子树，次小的作为左子树，将两个词的词频之和作为根的词频；后面将这棵树视作一个词，词频就是根的词频；<br>
重复上述步骤。

示例：
![霍夫曼树的构建](/docs/ml/images/13-1.jpg)

霍夫曼编码：基于霍夫曼树，向右走记为1,向左走记为0；从根节点到每个词（叶节点）的唯一路径便产生出唯一编码，并且不同的词对应不同的编码；
我：1，喜欢：000，观看：001，巴西：010，足球：0110，世界杯：0111

“我喜欢观看巴西足球世界杯”的编码为100000101001100111
