语言模型
====
### 词语的表示 ###
- onehot方式

词向量的维度为词汇表的大小，向量的每一位表示一个词；实际应用中容易带来维度灾难，另外也很难刻画词与词之间的相似程度。
- word vector方式

以固定大小，一般为100～300维的向量表示一个词，这里词向量是通过一定的方式训练出来的；这些词向量组成一个向量空间每一个词（词向量）是这个空间中的一个点，这样就可以引入空间距离来表示词的相似性；

具体来说，距离相近的词其空间距离应该也是相近的；词与词之间的差异性应该有一定的规律，或者说词与词之间构成的向量有相似性，比如中国与北京这两个点构成的向量应该和法国与巴黎构成的向量是一样或者近似的。

词向量是与语言模型捆绑在一起的，是语言模型训练结果的副产物；也就是通过神经网络模型训练的直接目的并不是得到词向量，而是为了做预测，比如一致某个词的上下文，预测这个词是什么（CBOW模型），或者是已知某个词，预测这个词的上下文等（skipgram），在这个训练过程中，产生了词向量。

### 语言模型 ###

语言模型就是为语句的联合概率函数进行建模，希望模型对有意义的语句赋予较大的概率，对没意义的语句赋予较小的概率；实际上就是语句的联合概率函数，下面介绍常见的集中语言模型。

- **n-gram模型**

n-gram模型是一种基于统计的语言模型，模型计算中所用参数都是基于已知语料统计来的。

假设由w1、w2、w3 ... wt组成的文本，其概率计算应该为 = ```P(w1)\*P(w2|w1)\*P(w3|w1w2)\*...\*P(wt|w1w2w3...wt-1)```

对于```P(wi|w1w2...wi-1)```的计算，等于```count(w1w2w3...wi)/count(w1w2w3...wi-1)``` ；

基于上述思想建模存在两个问题，数据稀疏严重；参数空间太大，无法实用

引入马尔可夫假设：一个词的出现，仅仅依赖于其前面的一个或者几个词；

1)假如不依赖于前面的词

概率为：```p(w1)\*p(w2)\*p(w3)\*p(w4)...p(wt)```

2)假如依赖于前面1个词（2-gram/bigram）

概率为：```P(w1)\*P(w2|w1)\*P(w3|w2)\*...\*P(wt|wt-1)```

3)假如依赖于前面的2个词（3-gram/trigram）

概率为：```P(w1)\*P(w2|w1)\*P(w3|w1w2)\*P(w4|w2w3)\*...\*P(wt|wt-2wt-1)```

实际使用中，为了避免数据溢出、提高性能，通常使用log后的加法运算代替乘法运算：<br>
```log(p(w1)\*p(w2)\*p(w3)\*p(w4)...p(wt)) = log(p(w1)) + log(p(w2)) + log(p(w3)) + ... + log(p(wt))```

如何计算模型的参数：



- **cbow**

- **skipgram**

### word2vec ###
- 霍夫曼编码

