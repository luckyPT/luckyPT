基于序列数据的预测
====
基于一段序列数据，预测这个序列之后可能出现的数据是什么。（重点是挖掘数据与数据之间存在的关系）<br>
训练时基本模型如下([参考官网](https://github.com/mari-linhares/docs/blob/patch-1/site/en/tutorials/sequences/images/text_generation_training.png?raw=true))：
![基本模型](/docs/python/images/5-1.jpg)

预测时使用的模型：([参考官网](https://github.com/mari-linhares/docs/blob/patch-1/site/en/tutorials/sequences/images/text_generation_sampling.png?raw=true))：<br>
![预测模型](/docs/python/images/5-2.jpg)

注意，预测时GRU的输入使用的是上一时刻的输出（如果前一时刻的真实值不知道）。

完整可运行的训练、预测代码如下（[参考](https://www.tensorflow.org/tutorials/sequences/text_generation)）：
```Python
import numpy as np
import tensorflow as tf

tf.enable_eager_execution()
text = open("../data/corpus.txt").read()
vocab = sorted(set(text))
char2idx = {u: i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)
text_as_int = np.array([char2idx[c] for c in text])

seq_length = 100
chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length + 1, drop_remainder=True)


def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text


BATCH_SIZE = 128
BUFFER_SIZE = 10000
dataset = chunks.map(split_input_target).shuffle(BUFFER_SIZE).batch(10 * BATCH_SIZE, drop_remainder=True)


def loss_function(real, preds):
    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)


def generate_text(model):
    start_string = 'Q'
    input_eval = [char2idx[s] for s in start_string]
    input_eval = tf.expand_dims(input_eval, 0)
    text_generated = []
    model.reset_states()
    for i in range(100):
        predictions = model.predict(input_eval, batch_size=1)
        predictions = tf.squeeze(predictions, 0)
        predictions /= 1.0
        predicted_id = tf.multinomial(predictions, num_samples=1)[-1, 0].numpy()
        input_eval = tf.expand_dims([predicted_id], 0)
        text_generated.append(idx2char[predicted_id])
    print(start_string + ''.join(text_generated))


checkpoint_dir = "../training_checkpoints/model.model"
vocab_size = len(vocab)
embedding_dim = 256
units = 1024

inputs = tf.keras.layers.Input(shape=(None,))
embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)
gru = tf.keras.layers.GRU(1024,
                          input_shape=[embedding_dim, ],
                          return_sequences=True,
                          recurrent_activation='sigmoid',
                          recurrent_initializer='glorot_uniform')(embedding)
prediction = tf.keras.layers.Dense(vocab_size)(gru)
model = tf.keras.Model(inputs=inputs, outputs=prediction)
# model = tf.keras.models.load_model(checkpoint_dir)
model.compile(loss=loss_function, optimizer=tf.train.AdamOptimizer())
EPOCHS = 5
for batch in range(EPOCHS):
    for (iters, (inp, target)) in enumerate(dataset):
        model.fit(inp, target, batch_size=BATCH_SIZE)
    model.save(checkpoint_dir)
    print(batch, '--complete')
    generate_text(model)

```
